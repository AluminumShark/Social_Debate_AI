# ğŸ“š Social Debate AI Training Guide

*English | [ä¸­æ–‡](#chinese-version)*

This guide provides detailed instructions on how to train the three core models of Social Debate AI: RAG, GNN, and RL.

## ğŸ“‹ Table of Contents

- [Environment Setup](#environment-setup)
- [Quick Training](#quick-training)
- [RAG System Training](#rag-system-training)
- [GNN Model Training](#gnn-model-training)
- [RL Model Training](#rl-model-training)
- [Training Monitoring](#training-monitoring)
- [FAQ](#faq)

## ğŸ”§ Environment Setup

### 1. Hardware Requirements
- **Minimum**: 8GB RAM, 4-core CPU
- **Recommended**: 16GB RAM, RTX 3060+ GPU, 8-core CPU
- **Storage**: At least 20GB (for data and models)

### 2. Environment Configuration
```bash
# Create virtual environment
conda create -n social_debate python=3.8
conda activate social_debate

# Install PyTorch (according to your CUDA version)
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CPU version
pip install torch torchvision torchaudio

# Install other dependencies
pip install -r requirements.txt
```

### 3. API Key Setup
```bash
# Create .env file
cp env.example .env

# Edit .env, add your OpenAI API Key
OPENAI_API_KEY=sk-your-api-key-here
```

## ğŸš€ Quick Training

### Train All Models (Recommended)
```bash
python train_all.py --all
```

This will train in sequence:
1. GNN social network model
2. RL strategy selection model
3. RAG retrieval index

Estimated total time: 30-60 minutes (depending on hardware)

### Individual Model Training
```bash
# Train GNN only
python train_all.py --gnn

# Train RL only
python train_all.py --rl

# Build RAG index only
python train_all.py --rag        # Simple index (fast)
python train_all.py --rag-chroma  # Chroma vector index (complete)
python train_all.py --rag-both    # Build both indexes
```

## ğŸ“š RAG System Training

RAG (Retrieval-Augmented Generation) system is responsible for retrieving relevant evidence to support debates.

### 1. Simple Index (Quick Test)
```bash
python train_all.py --rag
```

- **Processing Time**: ~5 minutes
- **Document Count**: 45,974 documents
- **Index Size**: ~50MB
- **Use Case**: Quick testing and development

### 2. Chroma Vector Index (Production)
```bash
python train_all.py --rag-chroma
```

- **Processing Time**: ~20-30 minutes
- **Document Chunks**: 94,525 chunks
- **Index Size**: ~500MB
- **Embedding Cost**: ~$0.02 (using OpenAI API)
- **Use Case**: Production environment, high-quality retrieval

### 3. Configuration
Edit `configs/rag.yaml`:

```yaml
chroma:
  embedding:
    batch_size: 500  # Increase batch size for faster processing
    model: "text-embedding-3-small"  # Optional: text-embedding-3-large

indexing:
  quality_filter:
    min_score: 10    # Minimum score filter
    min_length: 50   # Minimum text length
```

## ğŸ”— GNN Model Training

GNN (Graph Neural Network) model uses supervised learning to predict persuasion success rate and optimal strategies.

### 1. Training Command
```bash
python train_all.py --gnn
```

### 2. Training Architecture
- **Model Type**: GraphSAGE + GAT attention mechanism
- **Task Type**: Multi-task learning
  - Delta prediction (binary classification)
  - Quality scoring (regression)
  - Strategy classification (multi-class)
- **Training Data**: Delta/non-delta comments from CMV dataset
- **Training Time**: ~10-15 minutes (GPU)

### 3. Monitor Training
Training process displays:
```
Epoch 10/50, Loss: 1.2345, Delta Acc: 0.5678, Quality MAE: 2.3456, Strategy Acc: 0.4567
Epoch 20/50, Loss: 0.8901, Delta Acc: 0.6234, Quality MAE: 1.8901, Strategy Acc: 0.5678
Epoch 30/50, Loss: 0.5678, Delta Acc: 0.6789, Quality MAE: 1.4567, Strategy Acc: 0.6234
```

### 4. Training Performance
- **Delta Accuracy**: ~67-70%
- **Strategy Accuracy**: ~64-67%
- **Quality Prediction MAE**: ~1.2-1.5

### 5. Configuration Adjustment
Edit `configs/gnn.yaml`:

```yaml
training:
  epochs: 50           # Can increase to 100 for better results
  batch_size: 32       # Adjust based on GPU memory
  learning_rate: 0.001 # Adjustable learning rate
  
model:
  hidden_dim: 768      # BERT embedding dimension
  num_layers: 3        # Number of GNN layers
  dropout: 0.1         # Dropout rate
```

## ğŸ® RL Model Training

RL (Reinforcement Learning) model uses PPO algorithm to learn optimal debate strategies.

### 1. Training Command
```bash
python train_all.py --rl
```

### 2. PPO Training Architecture
- **Algorithm**: Proximal Policy Optimization (PPO)
- **Network Architecture**: Actor-Critic dual networks
- **State Space**: Text embeddings + stance + round + history
- **Action Space**: 4 strategies (aggressive, defensive, analytical, empathetic)
- **Training Time**: ~20-30 minutes (1000 episodes)

### 3. Environment Design
Debate environment simulates real interactions:
- **Initial State**: Random stance and belief
- **State Transition**: Based on strategy effectiveness
- **Reward Mechanism**:
  - Strategy effectiveness reward (-1 to +1)
  - Persuasion success reward (+5)
  - Strategy diversity reward (+0.1)
- **Termination**: Surrender or maximum rounds reached

### 4. Training Monitoring
```
Episode 100/1000, Avg Reward: -2.34, Policy Loss: 0.456, Value Loss: 1.234
Episode 200/1000, Avg Reward: -0.89, Policy Loss: 0.234, Value Loss: 0.789
Episode 500/1000, Avg Reward: 1.23, Policy Loss: 0.123, Value Loss: 0.456
Episode 1000/1000, Avg Reward: 2.56, Policy Loss: 0.089, Value Loss: 0.234
```

### 5. Configuration Optimization
Edit `configs/rl.yaml`:

```yaml
ppo:
  episodes: 1000       # Number of training episodes
  max_steps: 50        # Maximum steps per episode
  batch_size: 64       # Batch size
  learning_rate: 3e-4  # Learning rate
  gamma: 0.99          # Discount factor
  clip_epsilon: 0.2    # PPO clipping parameter
  
environment:
  reward_scale: 1.0    # Reward scaling
  persuasion_bonus: 5  # Persuasion success reward
  diversity_bonus: 0.1 # Strategy diversity reward
```

## ğŸ“Š Training Monitoring

### 1. Check Training Status
```bash
# View model files
ls -la data/models/

# Expected output:
# gnn_social.pt      (GNN model)
# policy/            (RL model directory)
# rag/               (RAG index)
```

### 2. Validate Models
```bash
# Run system integrity test
python test_system_integrity.py
```

### 3. GPU Usage Monitoring
```bash
# NVIDIA GPU
nvidia-smi -l 1

# Watch GPU memory usage
watch -n 1 nvidia-smi
```

## â“ FAQ

### Q1: CUDA out of memory
**Solution**:
- Reduce batch size: adjust `batch_size` in config files
- Use CPU training: install CPU version of PyTorch
- Use mixed precision training (automatically enabled)

### Q2: OpenAI API Error
**Solution**:
- Check if API Key is correctly set
- Confirm API quota is sufficient
- Use simple index instead of Chroma index

### Q3: Training takes too long
**Solution**:
- Use GPU acceleration
- Reduce training epochs
- Use pre-trained models

### Q4: Poor model performance
**Solution**:
- Increase training epochs
- Adjust learning rate
- Ensure data quality

## ğŸ¯ Training Recommendations

1. **First Use**: Train with default parameters to familiarize with the process
2. **Optimize Results**: Gradually adjust parameters and observe changes
3. **Production Deployment**: Use complete dataset and more training epochs
4. **Regular Updates**: Retrain periodically as new data accumulates

## ğŸ“ˆ Advanced Training

### 1. Custom Dataset
```python
# Prepare your data in JSONL format
# Place in data/raw/ directory
# Modify data paths in config files
```

### 2. Model Fine-tuning
```python
# Modify model architecture
# Edit src/gnn/social_encoder.py
# Edit src/rl/policy_network.py
```

### 3. Distributed Training
```bash
# Use multiple GPUs
CUDA_VISIBLE_DEVICES=0,1 python train_all.py --all
```

---

ğŸ’¡ **Tip**: If you encounter issues during training, check log files or submit an Issue.

ğŸ“§ **Support**: For assistance, contact your-email@example.com

---

## Chinese Version

# ğŸ“š Social Debate AI è¨“ç·´æŒ‡å—

*[English](#social-debate-ai-training-guide) | ä¸­æ–‡*

æœ¬æŒ‡å—è©³ç´°èªªæ˜å¦‚ä½•è¨“ç·´ Social Debate AI ç³»çµ±çš„ä¸‰å¤§æ ¸å¿ƒæ¨¡å‹ï¼šRAGã€GNN å’Œ RLã€‚

## ğŸ“‹ ç›®éŒ„

- [ç’°å¢ƒæº–å‚™](#ç’°å¢ƒæº–å‚™)
- [å¿«é€Ÿè¨“ç·´](#å¿«é€Ÿè¨“ç·´)
- [RAG ç³»çµ±è¨“ç·´](#rag-ç³»çµ±è¨“ç·´)
- [GNN æ¨¡å‹è¨“ç·´](#gnn-æ¨¡å‹è¨“ç·´)
- [RL æ¨¡å‹è¨“ç·´](#rl-æ¨¡å‹è¨“ç·´)
- [è¨“ç·´ç›£æ§](#è¨“ç·´ç›£æ§)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

## ğŸ”§ ç’°å¢ƒæº–å‚™

### 1. ç¡¬é«”è¦æ±‚
- **æœ€ä½é…ç½®**ï¼š8GB RAM, 4æ ¸ CPU
- **æ¨è–¦é…ç½®**ï¼š16GB RAM, RTX 3060+ GPU, 8æ ¸ CPU
- **ç£ç¢Ÿç©ºé–“**ï¼šè‡³å°‘ 20GBï¼ˆç”¨æ–¼æ•¸æ“šå’Œæ¨¡å‹ï¼‰

### 2. ç’°å¢ƒè¨­ç½®
```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ
conda create -n social_debate python=3.8
conda activate social_debate

# å®‰è£ PyTorch (æ ¹æ“šæ‚¨çš„ CUDA ç‰ˆæœ¬)
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CPU ç‰ˆæœ¬
pip install torch torchvision torchaudio

# å®‰è£å…¶ä»–ä¾è³´
pip install -r requirements.txt
```

### 3. API Key è¨­ç½®
```bash
# å‰µå»º .env æ–‡ä»¶
cp env.example .env

# ç·¨è¼¯ .envï¼Œæ·»åŠ æ‚¨çš„ OpenAI API Key
OPENAI_API_KEY=sk-your-api-key-here
```

## ğŸš€ å¿«é€Ÿè¨“ç·´

### è¨“ç·´æ‰€æœ‰æ¨¡å‹ï¼ˆæ¨è–¦ï¼‰
```bash
python train_all.py --all
```

é€™å°‡æŒ‰é †åºè¨“ç·´ï¼š
1. GNN ç¤¾æœƒç¶²çµ¡æ¨¡å‹
2. RL ç­–ç•¥é¸æ“‡æ¨¡å‹
3. RAG æª¢ç´¢ç´¢å¼•

é è¨ˆç¸½æ™‚é–“ï¼š30-60 åˆ†é˜ï¼ˆå–æ±ºæ–¼ç¡¬é«”ï¼‰

### å–®ç¨è¨“ç·´æ¨¡å‹
```bash
# åªè¨“ç·´ GNN
python train_all.py --gnn

# åªè¨“ç·´ RL
python train_all.py --rl

# åªæ§‹å»º RAG ç´¢å¼•
python train_all.py --rag        # ç°¡å–®ç´¢å¼•ï¼ˆå¿«é€Ÿï¼‰
python train_all.py --rag-chroma  # Chroma å‘é‡ç´¢å¼•ï¼ˆå®Œæ•´ï¼‰
python train_all.py --rag-both    # å…©ç¨®ç´¢å¼•éƒ½æ§‹å»º
```

## ğŸ“š RAG ç³»çµ±è¨“ç·´

RAG (Retrieval-Augmented Generation) ç³»çµ±è² è²¬æª¢ç´¢ç›¸é—œè­‰æ“šæ”¯æŒè¾¯è«–ã€‚

### 1. ç°¡å–®ç´¢å¼•ï¼ˆå¿«é€Ÿæ¸¬è©¦ï¼‰
```bash
python train_all.py --rag
```

- **è™•ç†æ™‚é–“**ï¼šç´„ 5 åˆ†é˜
- **æ–‡æª”æ•¸é‡**ï¼š45,974 å€‹
- **ç´¢å¼•å¤§å°**ï¼šç´„ 50MB
- **é©ç”¨å ´æ™¯**ï¼šå¿«é€Ÿæ¸¬è©¦å’Œé–‹ç™¼

### 2. Chroma å‘é‡ç´¢å¼•ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
```bash
python train_all.py --rag-chroma
```

- **è™•ç†æ™‚é–“**ï¼šç´„ 20-30 åˆ†é˜
- **æ–‡æª”ç‰‡æ®µ**ï¼š94,525 å€‹
- **ç´¢å¼•å¤§å°**ï¼šç´„ 500MB
- **åµŒå…¥æˆæœ¬**ï¼šç´„ $0.02ï¼ˆä½¿ç”¨ OpenAI APIï¼‰
- **é©ç”¨å ´æ™¯**ï¼šç”Ÿç”¢ç’°å¢ƒï¼Œé«˜è³ªé‡æª¢ç´¢

### 3. é…ç½®èªªæ˜
ç·¨è¼¯ `configs/rag.yaml`ï¼š

```yaml
chroma:
  embedding:
    batch_size: 500  # å¢åŠ æ‰¹æ¬¡å¤§å°åŠ å¿«è™•ç†
    model: "text-embedding-3-small"  # å¯é¸ text-embedding-3-large

indexing:
  quality_filter:
    min_score: 10    # æœ€ä½è©•åˆ†éæ¿¾
    min_length: 50   # æœ€çŸ­æ–‡æœ¬é•·åº¦
```

## ğŸ”— GNN æ¨¡å‹è¨“ç·´

GNN (Graph Neural Network) æ¨¡å‹ä½¿ç”¨ç›£ç£å¼å­¸ç¿’é æ¸¬èªªæœæˆåŠŸç‡å’Œæœ€ä½³ç­–ç•¥ã€‚

### 1. è¨“ç·´å‘½ä»¤
```bash
python train_all.py --gnn
```

### 2. è¨“ç·´æ¶æ§‹
- **æ¨¡å‹é¡å‹**ï¼šGraphSAGE + GAT æ³¨æ„åŠ›æ©Ÿåˆ¶
- **ä»»å‹™é¡å‹**ï¼šå¤šä»»å‹™å­¸ç¿’
  - Delta é æ¸¬ï¼ˆäºŒåˆ†é¡ï¼‰
  - å“è³ªè©•åˆ†ï¼ˆå›æ­¸ï¼‰
  - ç­–ç•¥åˆ†é¡ï¼ˆå¤šåˆ†é¡ï¼‰
- **è¨“ç·´æ•¸æ“š**ï¼šCMV æ•¸æ“šé›†çš„ delta/non-delta comments
- **è¨“ç·´æ™‚é–“**ï¼šç´„ 10-15 åˆ†é˜ï¼ˆGPUï¼‰

### 3. ç›£æ§è¨“ç·´
è¨“ç·´éç¨‹æœƒé¡¯ç¤ºï¼š
```
Epoch 10/50, Loss: 1.2345, Delta Acc: 0.5678, Quality MAE: 2.3456, Strategy Acc: 0.4567
Epoch 20/50, Loss: 0.8901, Delta Acc: 0.6234, Quality MAE: 1.8901, Strategy Acc: 0.5678
Epoch 30/50, Loss: 0.5678, Delta Acc: 0.6789, Quality MAE: 1.4567, Strategy Acc: 0.6234
```

### 4. è¨“ç·´æ•ˆæœ
- **Delta æº–ç¢ºç‡**ï¼šç´„ 67-70%
- **ç­–ç•¥æº–ç¢ºç‡**ï¼šç´„ 64-67%
- **å“è³ªé æ¸¬ MAE**ï¼šç´„ 1.2-1.5

### 5. é…ç½®èª¿æ•´
ç·¨è¼¯ `configs/gnn.yaml`ï¼š

```yaml
training:
  epochs: 50           # å¯å¢åŠ åˆ° 100 ä»¥ç²å¾—æ›´å¥½æ•ˆæœ
  batch_size: 32       # æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´
  learning_rate: 0.001 # å¯èª¿æ•´å­¸ç¿’ç‡
  
model:
  hidden_dim: 768      # BERT åµŒå…¥ç¶­åº¦
  num_layers: 3        # GNN å±¤æ•¸
  dropout: 0.1         # Dropout ç‡
```

## ğŸ® RL æ¨¡å‹è¨“ç·´

RL (Reinforcement Learning) æ¨¡å‹ä½¿ç”¨ PPO ç®—æ³•å­¸ç¿’æœ€ä½³è¾¯è«–ç­–ç•¥ã€‚

### 1. è¨“ç·´å‘½ä»¤
```bash
python train_all.py --rl
```

### 2. PPO è¨“ç·´æ¶æ§‹
- **ç®—æ³•**ï¼šProximal Policy Optimization (PPO)
- **ç¶²è·¯æ¶æ§‹**ï¼šActor-Critic é›™ç¶²è·¯
- **ç‹€æ…‹ç©ºé–“**ï¼šæ–‡æœ¬åµŒå…¥ + ç«‹å ´ + å›åˆ + æ­·å²
- **å‹•ä½œç©ºé–“**ï¼š4ç¨®ç­–ç•¥ï¼ˆaggressiveã€defensiveã€analyticalã€empatheticï¼‰
- **è¨“ç·´æ™‚é–“**ï¼šç´„ 20-30 åˆ†é˜ï¼ˆ1000 episodesï¼‰

### 3. ç’°å¢ƒè¨­è¨ˆ
è¾¯è«–ç’°å¢ƒæ¨¡æ“¬çœŸå¯¦äº’å‹•ï¼š
- **åˆå§‹ç‹€æ…‹**ï¼šéš¨æ©Ÿç«‹å ´å’Œä¿¡å¿µ
- **ç‹€æ…‹è½‰ç§»**ï¼šåŸºæ–¼ç­–ç•¥æ•ˆæœ
- **çå‹µæ©Ÿåˆ¶**ï¼š
  - ç­–ç•¥æ•ˆæœçå‹µï¼ˆ-1 åˆ° +1ï¼‰
  - èªªæœæˆåŠŸçå‹µï¼ˆ+5ï¼‰
  - ç­–ç•¥å¤šæ¨£æ€§çå‹µï¼ˆ+0.1ï¼‰
- **çµ‚æ­¢æ¢ä»¶**ï¼šæŠ•é™æˆ–é”åˆ°æœ€å¤§å›åˆ

### 4. è¨“ç·´ç›£æ§
```
Episode 100/1000, Avg Reward: -2.34, Policy Loss: 0.456, Value Loss: 1.234
Episode 200/1000, Avg Reward: -0.89, Policy Loss: 0.234, Value Loss: 0.789
Episode 500/1000, Avg Reward: 1.23, Policy Loss: 0.123, Value Loss: 0.456
Episode 1000/1000, Avg Reward: 2.56, Policy Loss: 0.089, Value Loss: 0.234
```

### 5. é…ç½®å„ªåŒ–
ç·¨è¼¯ `configs/rl.yaml`ï¼š

```yaml
ppo:
  episodes: 1000       # è¨“ç·´å›åˆæ•¸
  max_steps: 50        # æ¯å›åˆæœ€å¤§æ­¥æ•¸
  batch_size: 64       # æ‰¹æ¬¡å¤§å°
  learning_rate: 3e-4  # å­¸ç¿’ç‡
  gamma: 0.99          # æŠ˜æ‰£å› å­
  clip_epsilon: 0.2    # PPO è£å‰ªåƒæ•¸
  
environment:
  reward_scale: 1.0    # çå‹µç¸®æ”¾
  persuasion_bonus: 5  # èªªæœæˆåŠŸçå‹µ
  diversity_bonus: 0.1 # ç­–ç•¥å¤šæ¨£æ€§çå‹µ
```

## ğŸ“Š è¨“ç·´ç›£æ§

### 1. æª¢æŸ¥è¨“ç·´ç‹€æ…‹
```bash
# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶
ls -la data/models/

# é æœŸè¼¸å‡ºï¼š
# gnn_social.pt      (GNN æ¨¡å‹)
# policy/            (RL æ¨¡å‹ç›®éŒ„)
# rag/               (RAG ç´¢å¼•)
```

### 2. é©—è­‰æ¨¡å‹
```bash
# é‹è¡Œç³»çµ±æ•´åˆæ¸¬è©¦
python test_system_integrity.py
```

### 3. GPU ä½¿ç”¨ç›£æ§
```bash
# NVIDIA GPU
nvidia-smi -l 1

# æŸ¥çœ‹ GPU è¨˜æ†¶é«”ä½¿ç”¨
watch -n 1 nvidia-smi
```

## â“ å¸¸è¦‹å•é¡Œ

### Q1: CUDA out of memory
**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- æ¸›å°æ‰¹æ¬¡å¤§å°ï¼šåœ¨é…ç½®æ–‡ä»¶ä¸­èª¿æ•´ `batch_size`
- ä½¿ç”¨ CPU è¨“ç·´ï¼šå®‰è£ CPU ç‰ˆæœ¬çš„ PyTorch
- ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´ï¼ˆå·²è‡ªå‹•å•Ÿç”¨ï¼‰

### Q2: OpenAI API éŒ¯èª¤
**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢ºè¨­ç½®
- ç¢ºèª API é¡åº¦å……è¶³
- ä½¿ç”¨ç°¡å–®ç´¢å¼•ä»£æ›¿ Chroma ç´¢å¼•

### Q3: è¨“ç·´æ™‚é–“éé•·
**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ GPU åŠ é€Ÿ
- æ¸›å°‘è¨“ç·´è¼ªæ•¸
- ä½¿ç”¨é è¨“ç·´æ¨¡å‹

### Q4: æ¨¡å‹æ•ˆæœä¸ä½³
**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- å¢åŠ è¨“ç·´è¼ªæ•¸
- èª¿æ•´å­¸ç¿’ç‡
- ç¢ºä¿æ•¸æ“šè³ªé‡

## ğŸ¯ è¨“ç·´å»ºè­°

1. **é¦–æ¬¡ä½¿ç”¨**ï¼šå…ˆç”¨é»˜èªåƒæ•¸è¨“ç·´ï¼Œç†Ÿæ‚‰æµç¨‹
2. **å„ªåŒ–æ•ˆæœ**ï¼šé€æ­¥èª¿æ•´åƒæ•¸ï¼Œè§€å¯Ÿæ•ˆæœè®ŠåŒ–
3. **ç”Ÿç”¢éƒ¨ç½²**ï¼šä½¿ç”¨å®Œæ•´æ•¸æ“šé›†å’Œæ›´å¤šè¨“ç·´è¼ªæ•¸
4. **å®šæœŸæ›´æ–°**ï¼šéš¨è‘—æ–°æ•¸æ“šç©ç´¯ï¼Œå®šæœŸé‡æ–°è¨“ç·´

## ğŸ“ˆ é€²éšè¨“ç·´

### 1. è‡ªå®šç¾©æ•¸æ“šé›†
```python
# æº–å‚™æ‚¨çš„æ•¸æ“šç‚º JSONL æ ¼å¼
# æ”¾ç½®åœ¨ data/raw/ ç›®éŒ„
# ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„æ•¸æ“šè·¯å¾‘
```

### 2. æ¨¡å‹å¾®èª¿
```python
# ä¿®æ”¹æ¨¡å‹æ¶æ§‹
# ç·¨è¼¯ src/gnn/social_encoder.py
# ç·¨è¼¯ src/rl/policy_network.py
```

### 3. åˆ†æ•£å¼è¨“ç·´
```bash
# ä½¿ç”¨å¤š GPU
CUDA_VISIBLE_DEVICES=0,1 python train_all.py --all
```

---

ğŸ’¡ **æç¤º**ï¼šè¨“ç·´éç¨‹ä¸­å¦‚é‡åˆ°å•é¡Œï¼Œè«‹æŸ¥çœ‹æ—¥èªŒæ–‡ä»¶æˆ–æäº¤ Issueã€‚

ğŸ“§ **æ”¯æ´**ï¼šå¦‚éœ€å”åŠ©ï¼Œè«‹è¯ç¹« your-email@example.com 