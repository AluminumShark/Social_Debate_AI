"""
ç›£ç£å¼ GNN è¨“ç·´
é æ¸¬èªªæœæˆåŠŸç‡å’Œæœ€ä½³èªªæœç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric.nn as tgnn
from torch_geometric.data import Data, DataLoader
from pathlib import Path
import json
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import networkx as nx
from typing import Dict, List, Tuple
from transformers import AutoTokenizer, AutoModel

class PersuasionGNN(nn.Module):
    """èªªæœåŠ›é æ¸¬ GNN æ¨¡å‹"""
    
    def __init__(self, input_dim=768, hidden_dim=256, num_strategies=4):
        super().__init__()
        
        # åœ–å·ç©å±¤ï¼ˆä½¿ç”¨ GraphSAGEï¼‰
        self.conv1 = tgnn.SAGEConv(input_dim, hidden_dim)
        self.conv2 = tgnn.SAGEConv(hidden_dim, hidden_dim)
        self.conv3 = tgnn.SAGEConv(hidden_dim, hidden_dim // 2)
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.attention = tgnn.GATConv(hidden_dim // 2, hidden_dim // 2, heads=4, concat=False)
        
        # ä»»å‹™ç‰¹å®šçš„é æ¸¬é ­
        self.delta_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1)  # äºŒå…ƒåˆ†é¡ï¼šæ˜¯å¦ç‚º delta
        )
        
        self.quality_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1)  # å›æ­¸ï¼šèªªæœåŠ›åˆ†æ•¸
        )
        
        self.strategy_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, num_strategies)  # å¤šåˆ†é¡ï¼šæœ€ä½³ç­–ç•¥
        )
        
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x, edge_index, batch=None):
        # åœ–å·ç©
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)
        
        x = F.relu(self.conv2(x, edge_index))
        x = self.dropout(x)
        
        x = F.relu(self.conv3(x, edge_index))
        
        # æ³¨æ„åŠ›å±¤
        x = self.attention(x, edge_index)
        x = self.dropout(x)
        
        # å¦‚æœæœ‰æ‰¹æ¬¡ä¿¡æ¯ï¼Œé€²è¡Œå…¨å±€æ± åŒ–
        if batch is not None:
            x = tgnn.global_mean_pool(x, batch)
        
        # å¤šä»»å‹™é æ¸¬
        delta_pred = self.delta_head(x)
        quality_pred = self.quality_head(x)
        strategy_pred = self.strategy_head(x)
        
        return {
            'delta': delta_pred,
            'quality': quality_pred,
            'strategy': strategy_pred,
            'embeddings': x
        }

class PersuasionDataset:
    """èªªæœåŠ›æ•¸æ“šé›†"""
    
    def __init__(self, pairs_path='data/raw/pairs.jsonl'):
        self.pairs_path = Path(pairs_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"PersuasionDataset ä½¿ç”¨è¨­å‚™: {self.device}")
        
        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
        self.encoder = AutoModel.from_pretrained('distilbert-base-uncased')
        self.encoder = self.encoder.to(self.device)  # å°‡æ¨¡å‹ç§»åˆ° GPU
        self.encoder.eval()
        
        # ç­–ç•¥æ˜ å°„
        self.strategies = {
            'aggressive': 0,
            'defensive': 1,
            'analytical': 2,
            'empathetic': 3
        }
        
    def encode_text(self, text: str) -> np.ndarray:
        """ç·¨ç¢¼æ–‡æœ¬ç‚ºå‘é‡"""
        with torch.no_grad():
            inputs = self.tokenizer(text, truncation=True, max_length=512, 
                                  padding=True, return_tensors='pt')
            # å°‡è¼¸å…¥ç§»åˆ° GPU
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.encoder(**inputs)
            # ä½¿ç”¨ [CLS] token çš„è¡¨ç¤ºï¼Œä¸¦ç§»å› CPU
            return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()
    
    def encode_texts_batch(self, texts: List[str], batch_size: int = 16) -> List[np.ndarray]:
        """æ‰¹æ¬¡ç·¨ç¢¼æ–‡æœ¬ç‚ºå‘é‡ï¼ˆæé«˜ GPU åˆ©ç”¨ç‡ï¼‰"""
        all_embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                inputs = self.tokenizer(batch_texts, truncation=True, max_length=512, 
                                      padding=True, return_tensors='pt')
                # å°‡è¼¸å…¥ç§»åˆ° GPU
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                outputs = self.encoder(**inputs)
                # ä½¿ç”¨ [CLS] token çš„è¡¨ç¤º
                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                all_embeddings.extend(embeddings)
        
        return all_embeddings
    
    def extract_argument_features(self, text: str) -> Dict[str, float]:
        """æå–è«–è­‰ç‰¹å¾µ"""
        features = {}
        
        # è«–è­‰æŒ‡æ¨™è©
        argument_markers = {
            'causal': ['because', 'therefore', 'thus', 'hence', 'consequently'],
            'contrast': ['however', 'but', 'although', 'despite', 'nevertheless'],
            'evidence': ['studies show', 'research indicates', 'data suggests', 'according to'],
            'example': ['for example', 'for instance', 'such as', 'like'],
            'emphasis': ['indeed', 'in fact', 'clearly', 'obviously', 'certainly']
        }
        
        text_lower = text.lower()
        
        # è¨ˆç®—å„é¡è«–è­‰æ¨™è¨˜çš„å‡ºç¾æ¬¡æ•¸
        for category, markers in argument_markers.items():
            count = sum(1 for marker in markers if marker in text_lower)
            features[f'arg_{category}'] = count
        
        # è¨ˆç®—è«–è­‰çµæ§‹åˆ†æ•¸
        total_markers = sum(features.values())
        features['argument_density'] = total_markers / (len(text.split()) + 1)
        
        # å¥å­è¤‡é›œåº¦
        sentences = text.split('.')
        features['avg_sentence_length'] = np.mean([len(s.split()) for s in sentences if s.strip()])
        
        return features
    
    def determine_strategy(self, text: str, is_delta: bool) -> int:
        """æ ¹æ“šæ–‡æœ¬å…§å®¹åˆ¤æ–·èªªæœç­–ç•¥"""
        text_lower = text.lower()
        
        # ç­–ç•¥æŒ‡æ¨™è©
        aggressive_markers = ['wrong', 'incorrect', 'flawed', 'ridiculous', 'absurd']
        defensive_markers = ['defend', 'maintain', 'stand by', 'believe', 'position']
        analytical_markers = ['analyze', 'consider', 'examine', 'evaluate', 'assess']
        empathetic_markers = ['understand', 'appreciate', 'see your point', 'relate', 'feel']
        
        scores = {
            'aggressive': sum(1 for m in aggressive_markers if m in text_lower),
            'defensive': sum(1 for m in defensive_markers if m in text_lower),
            'analytical': sum(1 for m in analytical_markers if m in text_lower),
            'empathetic': sum(1 for m in empathetic_markers if m in text_lower)
        }
        
        # å¦‚æœæ˜¯æˆåŠŸçš„èªªæœï¼ˆdeltaï¼‰ï¼Œå‚¾å‘æ–¼åˆ†æå‹æˆ–åŒç†å‹
        if is_delta:
            scores['analytical'] += 2
            scores['empathetic'] += 1
        
        # é¸æ“‡å¾—åˆ†æœ€é«˜çš„ç­–ç•¥
        best_strategy = max(scores.keys(), key=lambda k: scores[k])
        return self.strategies[best_strategy]
    
    def build_interaction_graph(self) -> Tuple[Data, Dict]:
        """æ§‹å»ºäº’å‹•åœ–"""
        print("æ§‹å»ºèªªæœäº’å‹•åœ–...")
        
        # æ”¶é›†æ‰€æœ‰äº’å‹•æ•¸æ“š
        interactions = []
        texts_to_encode = []
        text_metadata = []  # å­˜å„²æ–‡æœ¬å°æ‡‰çš„å…ƒæ•¸æ“š
        
        print("æ”¶é›†æ–‡æœ¬æ•¸æ“š...")
        with open(self.pairs_path, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="æƒææ•¸æ“š"):
                try:
                    data = json.loads(line)
                    submission = data.get('submission', {})
                    
                    # è™•ç† delta è©•è«–
                    if 'delta_comment' in data and data['delta_comment']:
                        delta_data = data['delta_comment']
                        if 'comments' in delta_data and delta_data['comments']:
                            for comment in delta_data['comments']:
                                if comment.get('body') and len(comment['body']) > 50:
                                    author = comment.get('author', 'unknown')
                                    if author and author != '[deleted]' and author != 'unknown':
                                        texts_to_encode.append(comment['body'][:1000])
                                        text_metadata.append({
                                            'author': author,
                                            'is_delta': True,
                                            'score': comment.get('score', 0),
                                            'body': comment['body'],
                                            'op_author': submission.get('author', 'unknown')
                                        })
                    
                    # è™•ç† non-delta è©•è«–
                    if 'nodelta_comment' in data and data['nodelta_comment']:
                        nodelta_data = data['nodelta_comment']
                        if 'comments' in nodelta_data and nodelta_data['comments']:
                            for comment in nodelta_data['comments']:
                                if comment.get('body') and len(comment['body']) > 50:
                                    author = comment.get('author', 'unknown')
                                    if author and author != '[deleted]' and author != 'unknown':
                                        texts_to_encode.append(comment['body'][:1000])
                                        text_metadata.append({
                                            'author': author,
                                            'is_delta': False,
                                            'score': comment.get('score', 0),
                                            'body': comment['body'],
                                            'op_author': submission.get('author', 'unknown')
                                        })
                
                except Exception as e:
                    continue
        
        print(f"æ”¶é›†åˆ° {len(texts_to_encode)} æ¢æ–‡æœ¬")
        
        # æ‰¹æ¬¡ç·¨ç¢¼æ‰€æœ‰æ–‡æœ¬
        print("æ‰¹æ¬¡ç·¨ç¢¼æ–‡æœ¬ï¼ˆä½¿ç”¨ GPUï¼‰...")
        text_embeddings = self.encode_texts_batch(texts_to_encode, batch_size=32)
        
        # æ§‹å»ºç¯€é»ç‰¹å¾µå’Œæ¨™ç±¤
        node_features = {}
        node_labels = {}
        
        print("æ§‹å»ºç¯€é»ç‰¹å¾µ...")
        for embedding, metadata in zip(text_embeddings, text_metadata):
            author = metadata['author']
            
            # æå–è«–è­‰ç‰¹å¾µ
            arg_features = self.extract_argument_features(metadata['body'])
            
            # çµ„åˆç‰¹å¾µ
            features = np.concatenate([
                embedding,
                [arg_features.get('argument_density', 0),
                 arg_features.get('avg_sentence_length', 0)]
            ])
            
            node_features[author] = features
            node_labels[author] = {
                'is_delta': metadata['is_delta'],
                'score': metadata['score'],
                'strategy': self.determine_strategy(metadata['body'], metadata['is_delta'])
            }
            
            # æ·»åŠ äº’å‹•é‚Š
            op_author = metadata['op_author']
            if op_author and op_author != '[deleted]' and op_author != author:
                weight = 3.0 if metadata['is_delta'] else 1.0
                interactions.append((op_author, author, {'weight': weight}))
        
        # æ§‹å»ºåœ–
        G = nx.Graph()
        G.add_edges_from([(u, v) for u, v, _ in interactions])
        
        # å‰µå»ºç¯€é»ç´¢å¼•æ˜ å°„
        node_to_idx = {node: idx for idx, node in enumerate(G.nodes())}
        
        # æº–å‚™ PyG æ•¸æ“š
        x_list = []
        y_delta = []
        y_score = []
        y_strategy = []
        
        for node in G.nodes():
            if node in node_features:
                x_list.append(node_features[node])
                labels = node_labels[node]
                y_delta.append(1 if labels['is_delta'] else 0)
                y_score.append(labels['score'] / 100.0)  # æ¨™æº–åŒ–åˆ†æ•¸
                y_strategy.append(labels['strategy'])
            else:
                # å°æ–¼æ²’æœ‰ç‰¹å¾µçš„ç¯€é»ï¼Œä½¿ç”¨é›¶å‘é‡
                x_list.append(np.zeros(770))  # 768 + 2
                y_delta.append(0)
                y_score.append(0.0)
                y_strategy.append(2)  # é è¨­ç‚º analytical
        
        # è½‰æ›ç‚ºå¼µé‡ - å…ˆè½‰æ›ç‚º numpy array å†è½‰ç‚ºå¼µé‡
        x = torch.FloatTensor(np.array(x_list))
        edge_index = torch.LongTensor([[node_to_idx[u], node_to_idx[v]] 
                                      for u, v in G.edges()]).t()
        
        # å‰µå»º PyG Data å°è±¡
        data = Data(
            x=x,
            edge_index=edge_index,
            y_delta=torch.FloatTensor(y_delta),
            y_score=torch.FloatTensor(y_score),
            y_strategy=torch.LongTensor(y_strategy)
        )
        
        print(f"åœ–æ§‹å»ºå®Œæˆï¼š{len(G.nodes())} å€‹ç¯€é»ï¼Œ{len(G.edges())} æ¢é‚Š")
        print(f"Delta ç¯€é»ï¼š{sum(y_delta)} å€‹")
        
        return data, node_to_idx

def train_supervised_gnn(
    epochs=50,
    hidden_dim=256,
    learning_rate=0.001,
    output_path='data/models/gnn_persuasion.pt'
):
    """è¨“ç·´ç›£ç£å¼ GNN"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # æº–å‚™æ•¸æ“š
    dataset = PersuasionDataset()
    data, node_to_idx = dataset.build_interaction_graph()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„æ•¸æ“š
    if data.x.size(0) == 0:
        print("âŒ éŒ¯èª¤ï¼šæ²’æœ‰è¼‰å…¥åˆ°ä»»ä½•æ•¸æ“šï¼")
        print("è«‹ç¢ºèª data/raw/pairs.jsonl æª”æ¡ˆå­˜åœ¨ä¸”æ ¼å¼æ­£ç¢ºã€‚")
        return
    
    if data.x.size(0) < 10:
        print(f"âš ï¸ è­¦å‘Šï¼šåªæœ‰ {data.x.size(0)} å€‹ç¯€é»ï¼Œå¯èƒ½ä¸è¶³ä»¥é€²è¡Œæœ‰æ•ˆè¨“ç·´ã€‚")
    
    data = data.to(device)
    
    # åŠƒåˆ†è¨“ç·´é›†å’Œæ¸¬è©¦é›†
    num_nodes = data.x.size(0)
    indices = np.arange(num_nodes)
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
    
    train_mask = torch.zeros(num_nodes, dtype=torch.bool)
    test_mask = torch.zeros(num_nodes, dtype=torch.bool)
    train_mask[train_idx] = True
    test_mask[test_idx] = True
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = PersuasionGNN(
        input_dim=data.x.size(1),
        hidden_dim=hidden_dim
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    # è¨“ç·´
    print(f"\né–‹å§‹è¨“ç·´ {epochs} å€‹ epochs...")
    best_val_acc = 0
    
    for epoch in range(epochs):
        # è¨“ç·´æ¨¡å¼
        model.train()
        optimizer.zero_grad()
        
        outputs = model(data.x, data.edge_index)
        
        # è¨ˆç®—å¤šä»»å‹™æå¤±
        # 1. Delta é æ¸¬æå¤±ï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰
        delta_loss = F.binary_cross_entropy_with_logits(
            outputs['delta'][train_mask].squeeze(),
            data.y_delta[train_mask]
        )
        
        # 2. å“è³ªåˆ†æ•¸æå¤±ï¼ˆMSEï¼‰
        quality_loss = F.mse_loss(
            outputs['quality'][train_mask].squeeze(),
            data.y_score[train_mask]
        )
        
        # 3. ç­–ç•¥é æ¸¬æå¤±ï¼ˆäº¤å‰ç†µï¼‰
        strategy_loss = F.cross_entropy(
            outputs['strategy'][train_mask],
            data.y_strategy[train_mask]
        )
        
        # ç¸½æå¤±ï¼ˆåŠ æ¬Šçµ„åˆï¼‰
        total_loss = 0.5 * delta_loss + 0.3 * quality_loss + 0.2 * strategy_loss
        
        total_loss.backward()
        optimizer.step()
        
        # è©•ä¼°
        if (epoch + 1) % 5 == 0:
            model.eval()
            with torch.no_grad():
                outputs = model(data.x, data.edge_index)
                
                # Delta é æ¸¬æº–ç¢ºç‡
                delta_pred = (outputs['delta'][test_mask].squeeze() > 0).float()
                delta_acc = accuracy_score(
                    data.y_delta[test_mask].cpu(),
                    delta_pred.cpu()
                )
                
                # ç­–ç•¥é æ¸¬æº–ç¢ºç‡
                strategy_pred = outputs['strategy'][test_mask].argmax(dim=1)
                strategy_acc = accuracy_score(
                    data.y_strategy[test_mask].cpu(),
                    strategy_pred.cpu()
                )
                
                print(f"Epoch {epoch+1}/{epochs}")
                print(f"  è¨“ç·´æå¤±: {total_loss.item():.4f}")
                print(f"  Delta æº–ç¢ºç‡: {delta_acc:.4f}")
                print(f"  ç­–ç•¥æº–ç¢ºç‡: {strategy_acc:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if delta_acc > best_val_acc:
                    best_val_acc = delta_acc
                    
                    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
                    output_dir = Path(output_path).parent
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    torch.save({
                        'model_state': model.state_dict(),
                        'node_to_idx': node_to_idx,
                        'config': {
                            'hidden_dim': hidden_dim,
                            'input_dim': data.x.size(1)
                        },
                        'performance': {
                            'delta_acc': delta_acc,
                            'strategy_acc': strategy_acc
                        }
                    }, output_path)
                    print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Delta æº–ç¢ºç‡: {delta_acc:.4f})")
    
    print(f"\nâœ… è¨“ç·´å®Œæˆï¼æœ€ä½³ Delta æº–ç¢ºç‡: {best_val_acc:.4f}")

if __name__ == "__main__":
    train_supervised_gnn() 