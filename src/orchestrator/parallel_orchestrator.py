"""
å¹³è¡Œè™•ç†è¾¯è«–å”èª¿å™¨
æ”¯æ´ RL + GNN + RAG å¹³è¡Œé‹è¡Œï¼Œå‹•æ…‹èªªæœ/åé§æ©Ÿåˆ¶
"""

import asyncio
import concurrent.futures
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import json
import time
from pathlib import Path

try:
    from ..rl.policy_network import select_strategy, choose_snippet, PolicyNetwork
    from ..gnn.social_encoder import social_vec
<<<<<<< Updated upstream
    from ..rag.retriever import create_enhanced_retriever
    from ..gpt_interface.gpt_client import chat
except ImportError:
    # å›é€€å°å…¥
    import sys
    sys.path.append(str(Path(__file__).parent.parent))
    from rl.policy_network import select_strategy, choose_snippet, PolicyNetwork
    from gnn.social_encoder import social_vec
    from rag.retriever import create_enhanced_retriever
    from gpt_interface.gpt_client import chat
=======
    from ..rag.retriever import create_retriever
    from ..utils.config_loader import ConfigLoader
    from ..gpt_interface.gpt_client import chat
except ImportError:
    # å‚™ç”¨å°å…¥è·¯å¾‘
    try:
        from rl.policy_network import select_strategy, choose_snippet, PolicyNetwork
        from gnn.social_encoder import social_vec
        from rag.retriever import create_retriever
        from utils.config_loader import ConfigLoader
        from gpt_interface.gpt_client import chat
    except ImportError as e:
        print(f"âš ï¸ å°å…¥éŒ¯èª¤: {e}")
        # å®šç¾©è™›æ“¬å‡½æ•¸ä»¥é¿å…éŒ¯èª¤
        PolicyNetwork = None
        social_vec = lambda x: [0.1] * 128
        create_retriever = None
        ConfigLoader = None
        # å®šç¾©è™›æ“¬ chat å‡½æ•¸
        def chat(prompt: str) -> str:
            """è™›æ“¬ chat å‡½æ•¸ï¼Œç•¶ GPT ä¸å¯ç”¨æ™‚ä½¿ç”¨"""
            strategies = ['analytical', 'aggressive', 'defensive', 'empathetic']
            import random
            strategy = random.choice(strategies)
            return f"[æ¨¡æ“¬å›æ‡‰ - {strategy}ç­–ç•¥] åŸºæ–¼'{prompt[:50]}...'çš„åˆ†æï¼Œæˆ‘èªç‚ºé€™å€‹è­°é¡Œéœ€è¦æ›´æ·±å…¥çš„è¨è«–ã€‚"
>>>>>>> Stashed changes

@dataclass
class AgentState:
    """Agent ç‹€æ…‹"""
    agent_id: str
    current_stance: float  # -1.0 åˆ° 1.0ï¼Œç«‹å ´å¼·åº¦
    conviction: float      # 0.0 åˆ° 1.0ï¼Œä¿¡å¿µå …å®šåº¦
    social_context: List[float]  # ç¤¾æœƒèƒŒæ™¯å‘é‡
    persuasion_history: List[float]  # è¢«èªªæœæ­·å²
    attack_history: List[float]     # æ”»æ“Šæ­·å²
    has_surrendered: bool = False  # æ˜¯å¦å·²æŠ•é™
    
    def update_stance(self, persuasion_score: float, attack_score: float):
        """æ›´æ–°ç«‹å ´å’Œä¿¡å¿µ"""
        # è¨ˆç®—èªªæœæ•ˆæœ
        persuasion_effect = persuasion_score * (1.0 - self.conviction)
        
        # è¨ˆç®—æ”»æ“ŠæŠµæŠ—
        attack_resistance = self.conviction * 0.8
        attack_effect = max(0, attack_score - attack_resistance)
        
        # æ›´æ–°ç«‹å ´ (èªªæœä½¿ç«‹å ´è¶¨å‘ä¸­æ€§ï¼Œæ”»æ“Šä½¿ç«‹å ´æ¥µåŒ–)
        if persuasion_score > 0.6:  # è¢«èªªæœ
            self.current_stance *= (1.0 - persuasion_effect * 0.3)
            self.conviction *= 0.85  # ä¿¡å¿µæ¸›å¼±æ›´å¤š
        
        if attack_effect > 0.3:  # è¢«æ”»æ“Š
            self.current_stance *= (1.0 + attack_effect * 0.2)  # ç«‹å ´æ›´æ¥µç«¯
            self.conviction = min(1.0, self.conviction * 1.1)  # ä¿¡å¿µå¢å¼·
        
        # è¨˜éŒ„æ­·å²
        self.persuasion_history.append(persuasion_score)
        self.attack_history.append(attack_score)
        
        # ä¿æŒæ­·å²é•·åº¦
        if len(self.persuasion_history) > 10:
            self.persuasion_history.pop(0)
        if len(self.attack_history) > 10:
            self.attack_history.pop(0)
        
        # æª¢æŸ¥æ˜¯å¦æ‡‰è©²æŠ•é™ï¼ˆæ¢ä»¶æ”¾å¯¬ï¼‰
        if len(self.persuasion_history) >= 2:
            recent_persuasion = sum(self.persuasion_history[-2:]) / 2
            # æ¢ä»¶1ï¼šé«˜èªªæœåº¦ + ä½ä¿¡å¿µ
            if recent_persuasion > 0.6 and self.conviction < 0.4:
                self.has_surrendered = True
                print(f"ğŸ’” {self.agent_id} è¢«èªªæœäº†ï¼Œé¸æ“‡æŠ•é™ï¼")
            # æ¢ä»¶2ï¼šç«‹å ´å·²ç¶“æ¥è¿‘ä¸­ç«‹
            elif abs(self.current_stance) < 0.2 and self.conviction < 0.5:
                self.has_surrendered = True
                print(f"ğŸ’” {self.agent_id} ç«‹å ´å‹•æ–ï¼Œé¸æ“‡æŠ•é™ï¼")
            # æ¢ä»¶3ï¼šé€£çºŒè¢«é«˜åº¦èªªæœ
            elif len(self.persuasion_history) >= 3:
                consecutive_high = all(score > 0.5 for score in self.persuasion_history[-3:])
                if consecutive_high:
                    self.has_surrendered = True
                    print(f"ğŸ’” {self.agent_id} é€£çºŒè¢«èªªæœï¼Œé¸æ“‡æŠ•é™ï¼")

@dataclass
class DebateRound:
    """è¾¯è«–å›åˆ"""
    round_number: int
    topic: str
    agent_states: Dict[str, AgentState]
    history: List[Dict]
    
class ParallelOrchestrator:
    """å¹³è¡Œè™•ç†è¾¯è«–å”èª¿å™¨"""
    
    def __init__(self):
        self.policy_network = PolicyNetwork()
        self.retriever = create_enhanced_retriever()
        self.agent_states = {}
        self.debate_history = []
        
        # åŸ·è¡Œå™¨æ± 
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
        
        print("âœ… å¹³è¡Œè¾¯è«–å”èª¿å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def initialize_agents(self, agent_configs: List[Dict]) -> Dict[str, AgentState]:
        """åˆå§‹åŒ– Agent ç‹€æ…‹"""
        agents = {}
        
        for config in agent_configs:
            agent_id = config['id']
            agents[agent_id] = AgentState(
                agent_id=agent_id,
                current_stance=config.get('initial_stance', 0.0),
                conviction=config.get('initial_conviction', 0.7),
                social_context=config.get('social_context', [0.0] * 128),
                persuasion_history=[],
                attack_history=[]
            )
        
        self.agent_states = agents
        print(f"âœ… åˆå§‹åŒ– {len(agents)} å€‹ Agent")
        return agents
    
    async def parallel_analysis(self, agent_id: str, topic: str, 
                              history: List[str]) -> Dict:
        """å¹³è¡ŒåŸ·è¡Œ RL + GNN + RAG åˆ†æ"""
        
        # æ§‹å»ºæŸ¥è©¢ä¸Šä¸‹æ–‡
        recent_turns = history[-3:] if history else []
        context = f"Topic: {topic}\nRecent: {' '.join(recent_turns)}"
        agent_state = self.agent_states[agent_id]
        
        # å‰µå»ºç•°æ­¥ä»»å‹™
        loop = asyncio.get_event_loop()
        
        # 1. RL ç­–ç•¥é¸æ“‡
        rl_task = loop.run_in_executor(
            self.executor,
            self._rl_analysis,
            context, agent_state.social_context
        )
        
        # 2. GNN ç¤¾æœƒåˆ†æ
        gnn_task = loop.run_in_executor(
            self.executor,
            self._gnn_analysis,
            agent_id, agent_state
        )
        
        # 3. RAG è­‰æ“šæª¢ç´¢
        rag_task = loop.run_in_executor(
            self.executor,
            self._rag_analysis,
            context, topic
        )
        
        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        try:
            rl_result, gnn_result, rag_result = await asyncio.gather(
                rl_task, gnn_task, rag_task
            )
            
            return {
                'rl': rl_result,
                'gnn': gnn_result,
                'rag': rag_result,
                'timestamp': time.time()
            }
        except Exception as e:
            print(f"âŒ å¹³è¡Œåˆ†æå¤±æ•—: {e}")
            return self._fallback_analysis(context, agent_id)
    
    def _rl_analysis(self, context: str, social_context: List[float]) -> Dict:
        """RL ç­–ç•¥åˆ†æ"""
        try:
            strategy = select_strategy(context, "", social_context)
            print(f"  ğŸ“Š RL: é¸æ“‡ç­–ç•¥ = {strategy}")
            
            # é æ¸¬å“è³ªåˆ†æ•¸
            quality_score = self.policy_network.predict_quality(context)
            print(f"  ğŸ“Š RL: å“è³ªåˆ†æ•¸ = {quality_score:.2f}")
            
            return {
                'strategy': strategy,
                'quality_score': quality_score,
                'confidence': 0.8  # å¯ä»¥å¾æ¨¡å‹ç²å–
            }
        except Exception as e:
            print(f"âš ï¸ RL åˆ†æå¤±æ•—: {e}")
            return {'strategy': 'analytical', 'quality_score': 0.5, 'confidence': 0.3}
    
    def _gnn_analysis(self, agent_id: str, agent_state: AgentState) -> Dict:
        """GNN ç¤¾æœƒé—œä¿‚åˆ†æ"""
        try:
            # ç²å–ç¤¾æœƒå‘é‡
            social_vector = social_vec(agent_id)
            print(f"  ğŸŒ GNN: ç¤¾æœƒå‘é‡ç¶­åº¦ = {len(social_vector)}")
            
            # åˆ†æç¤¾æœƒå½±éŸ¿åŠ›
            influence_score = sum(social_vector[:10]) / 10  # ç°¡åŒ–è¨ˆç®—
            print(f"  ğŸŒ GNN: å½±éŸ¿åŠ›åˆ†æ•¸ = {influence_score:.2f}")
            
            # åˆ†æç«‹å ´è®ŠåŒ–è¶¨å‹¢
            stance_trend = 0.0
            if len(agent_state.persuasion_history) >= 2:
                recent_persuasion = sum(agent_state.persuasion_history[-3:]) / 3
                stance_trend = recent_persuasion - 0.5
            
            print(f"  ğŸŒ GNN: ç«‹å ´è¶¨å‹¢ = {stance_trend:.2f}")
            
            return {
                'social_vector': social_vector,
                'influence_score': influence_score,
                'stance_trend': stance_trend,
                'current_stance': agent_state.current_stance,
                'conviction': agent_state.conviction
            }
        except Exception as e:
            print(f"âš ï¸ GNN åˆ†æå¤±æ•—: {e}")
            return {
                'social_vector': [0.0] * 128,
                'influence_score': 0.5,
                'stance_trend': 0.0,
                'current_stance': agent_state.current_stance,
                'conviction': agent_state.conviction
            }
    
    def _rag_analysis(self, context: str, topic: str) -> Dict:
        """RAG è­‰æ“šæª¢ç´¢åˆ†æ"""
        try:
            # æª¢ç´¢è­‰æ“šæ± 
            retrieval_results = self.retriever.retrieve(
                query=context,
                top_k=8
            )
            print(f"  ğŸ“š RAG: æª¢ç´¢åˆ° {len(retrieval_results)} å€‹è­‰æ“š")
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼ä¾› choose_snippet ä½¿ç”¨
            evidence_pool = []
            for result in retrieval_results:
                evidence_dict = {
                    'content': result.content,
                    'similarity_score': result.score,
                    'metadata': result.metadata,
                    'doc_id': result.doc_id
                }
                evidence_pool.append(evidence_dict)
            
            # é¸æ“‡æœ€ä½³è­‰æ“š
            best_evidence = choose_snippet(context, evidence_pool)
            print(f"  ğŸ“š RAG: æœ€ä½³è­‰æ“šé•·åº¦ = {len(best_evidence)} å­—")
            
            # åˆ†æè­‰æ“šé¡å‹åˆ†å¸ƒ
            evidence_types = {}
            for result in retrieval_results:
                ev_type = result.metadata.get('type', 'unknown')
                evidence_types[ev_type] = evidence_types.get(ev_type, 0) + 1
            
            print(f"  ğŸ“š RAG: è­‰æ“šé¡å‹åˆ†å¸ƒ = {evidence_types}")
            
            return {
                'evidence_pool': evidence_pool,
                'best_evidence': best_evidence,
                'evidence_types': evidence_types,
                'total_evidence': len(evidence_pool)
            }
        except Exception as e:
            print(f"âš ï¸ RAG åˆ†æå¤±æ•—: {e}")
            return {
                'evidence_pool': [],
                'best_evidence': "No evidence available",
                'evidence_types': {},
                'total_evidence': 0
            }
    
    def _fallback_analysis(self, context: str, agent_id: str) -> Dict:
        """å›é€€åˆ†æ"""
        return {
            'rl': {'strategy': 'analytical', 'quality_score': 0.5, 'confidence': 0.3},
            'gnn': {'social_vector': [0.0] * 128, 'influence_score': 0.5, 
                   'stance_trend': 0.0, 'current_stance': 0.0, 'conviction': 0.7},
            'rag': {'evidence_pool': [], 'best_evidence': "No evidence available",
                   'evidence_types': {}, 'total_evidence': 0},
            'timestamp': time.time()
        }
    
    def fuse_analysis_results(self, analysis_results: Dict, agent_id: str) -> Dict:
        """èåˆåˆ†æçµæœ"""
        rl_result = analysis_results['rl']
        gnn_result = analysis_results['gnn']
        rag_result = analysis_results['rag']
        
        # ç­–ç•¥èª¿æ•´ï¼šæ ¹æ“šç¤¾æœƒå½±éŸ¿åŠ›å’Œç«‹å ´èª¿æ•´ç­–ç•¥
        base_strategy = rl_result['strategy']
        influence_score = gnn_result['influence_score']
        current_stance = gnn_result['current_stance']
        
        # é«˜å½±éŸ¿åŠ› + å¼·ç«‹å ´ = æ›´ç©æ¥µ
        if influence_score > 0.6 and abs(current_stance) > 0.5:
            if base_strategy == 'analytical':
                adjusted_strategy = 'aggressive'
                print(f"  ğŸ”„ ç­–ç•¥èª¿æ•´: {base_strategy} â†’ {adjusted_strategy} (é«˜å½±éŸ¿åŠ›+å¼·ç«‹å ´)")
            else:
                adjusted_strategy = base_strategy
        # ä½å½±éŸ¿åŠ› + å¼±ç«‹å ´ = æ›´è¬¹æ…
        elif influence_score < 0.4 and abs(current_stance) < 0.3:
            if base_strategy == 'aggressive':
                adjusted_strategy = 'defensive'
                print(f"  ğŸ”„ ç­–ç•¥èª¿æ•´: {base_strategy} â†’ {adjusted_strategy} (ä½å½±éŸ¿åŠ›+å¼±ç«‹å ´)")
            else:
                adjusted_strategy = base_strategy
        else:
            adjusted_strategy = base_strategy
        
        # è­‰æ“šé¸æ“‡ï¼šæ ¹æ“šç­–ç•¥èª¿æ•´è­‰æ“šé¸æ“‡
        evidence = rag_result['best_evidence']
        evidence_confidence = min(1.0, rag_result['total_evidence'] / 5.0)
        
        print(f"  âœ¨ èåˆçµæœ: æœ€çµ‚ç­–ç•¥={adjusted_strategy}, è­‰æ“šä¿¡å¿ƒ={evidence_confidence:.2f}")
        
        return {
            'final_strategy': adjusted_strategy,
            'evidence': evidence,
            'evidence_confidence': evidence_confidence,
            'social_influence': influence_score,
            'stance_strength': abs(current_stance),
            'conviction': gnn_result['conviction'],
            'fusion_timestamp': time.time()
        }
    
    async def generate_response(self, agent_id: str, topic: str, 
                              history: List[str], target_agents: List[str]) -> str:
        """ç”Ÿæˆè¾¯è«–å›è¦†"""
        
        # 1. å¹³è¡Œåˆ†æ
        print(f"ğŸ”„ Agent {agent_id} é–‹å§‹å¹³è¡Œåˆ†æ...")
        analysis_start = time.time()
        
        analysis_results = await self.parallel_analysis(agent_id, topic, history)
        
        analysis_time = time.time() - analysis_start
        print(f"âš¡ å¹³è¡Œåˆ†æå®Œæˆ ({analysis_time:.2f}s)")
        
        # 2. èåˆçµæœ
        fused_results = self.fuse_analysis_results(analysis_results, agent_id)
        
        # 3. æ§‹å»ºæç¤º
        agent_state = self.agent_states[agent_id]
        recent_history = history[-4:] if history else []
        
        # åˆ†æç›®æ¨™ Agent çš„å¼±é»
        target_analysis = self._analyze_targets(target_agents, history)
        
        prompt = self._build_enhanced_prompt(
            agent_id, topic, recent_history, fused_results, target_analysis
        )
        
        # 4. ç”Ÿæˆå›è¦†
        print(f"ğŸ¤– Agent {agent_id} ä½¿ç”¨ {fused_results['final_strategy']} ç­–ç•¥ç”Ÿæˆå›è¦†...")
        response = chat(prompt)
        
        # æª¢æŸ¥å›æ‡‰æ˜¯å¦è¢«æˆªæ–·ï¼ˆæª¢æŸ¥æ˜¯å¦ä»¥å¥è™Ÿã€å•è™Ÿæˆ–é©šå˜†è™Ÿçµå°¾ï¼‰
        if response and not response.rstrip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            print(f"âš ï¸ æª¢æ¸¬åˆ°å›æ‡‰å¯èƒ½è¢«æˆªæ–·ï¼Œå˜—è©¦è£œå……å®Œæ•´...")
            # å¦‚æœå›æ‡‰è¢«æˆªæ–·ï¼Œæ·»åŠ çµå°¾
            response += "ã€‚ç¸½ä¹‹ï¼ŒåŸºæ–¼ä»¥ä¸Šåˆ†æï¼Œæˆ‘å …æŒæˆ‘çš„ç«‹å ´ã€‚"
        
        # 5. è©•ä¼°å›è¦†æ•ˆæœ
        response_effects = self._evaluate_response(response, target_agents)
        
        return response
    
    def _analyze_targets(self, target_agents: List[str], history: List[str]) -> Dict:
        """åˆ†æç›®æ¨™ Agent çš„å¼±é»å’Œæ©Ÿæœƒ"""
        target_analysis = {}
        
        for target_id in target_agents:
            if target_id in self.agent_states:
                target_state = self.agent_states[target_id]
                
                # åˆ†æèªªæœæ©Ÿæœƒ
                persuasion_opportunity = 1.0 - target_state.conviction
                
                # åˆ†ææ”»æ“Šæ©Ÿæœƒ
                attack_opportunity = abs(target_state.current_stance)
                
                # åˆ†ææ­·å²è¶¨å‹¢
                recent_persuasion = 0.0
                if target_state.persuasion_history:
                    recent_persuasion = sum(target_state.persuasion_history[-2:]) / 2
                
                target_analysis[target_id] = {
                    'persuasion_opportunity': persuasion_opportunity,
                    'attack_opportunity': attack_opportunity,
                    'recent_persuasion': recent_persuasion,
                    'stance': target_state.current_stance,
                    'conviction': target_state.conviction
                }
        
        return target_analysis
    
    def _build_enhanced_prompt(self, agent_id: str, topic: str, history: List[str],
                              fused_results: Dict, target_analysis: Dict) -> str:
        """æ§‹å»ºå¢å¼·æç¤º"""
        
        agent_state = self.agent_states[agent_id]
        strategy = fused_results['final_strategy']
        evidence = fused_results['evidence']
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºç¬¬ä¸€å›åˆ
        is_first_round = len(history) == 0
        
        # æ­·å²å°è©±
        history_text = '\n'.join(f"Turn {i+1}: {turn}" for i, turn in enumerate(history))
        
        # åˆ†æå·²ä½¿ç”¨çš„è«–é»ï¼ˆé¿å…é‡è¤‡ï¼‰
        used_arguments = self._extract_used_arguments(history, agent_id)
        
        # ç›®æ¨™åˆ†æï¼ˆç¬¬ä¸€å›åˆä¸éœ€è¦ï¼‰
        target_info = ""
        if not is_first_round:
            for target_id, analysis in target_analysis.items():
                target_info += f"\n- {target_id}: ç«‹å ´{analysis['stance']:.2f}, ä¿¡å¿µ{analysis['conviction']:.2f}, èªªæœæ©Ÿæœƒ{analysis['persuasion_opportunity']:.2f}"
        
        # å¢å¼·çš„ç­–ç•¥æŒ‡å°
        strategy_guidance = {
            'aggressive': "æ¡ç”¨æ‰¹åˆ¤æ€§åˆ†æç­–ç•¥ï¼šæ·±å…¥å‰–æå°æ–¹è«–é»çš„é‚è¼¯ç¼ºé™·ï¼Œç”¨æœ‰åŠ›çš„åä¾‹å’Œæ•¸æ“šæŒ‘æˆ°å…¶æ ¸å¿ƒå‡è¨­ã€‚",
            'defensive': "æ¡ç”¨ç©©å¥è«–è­‰ç­–ç•¥ï¼šéå›ºè‡ªå·±çš„æ ¸å¿ƒè«–é»ï¼Œç³»çµ±æ€§åœ°å›æ‡‰è³ªç–‘ï¼Œç”¨æ›´å¤šè­‰æ“šå¼·åŒ–ç«‹å ´ã€‚",
            'analytical': "æ¡ç”¨ç†æ€§åˆ†æç­–ç•¥ï¼šé‹ç”¨é‚è¼¯æ¨ç†ã€å¯¦è­‰æ•¸æ“šå’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œå®¢è§€åœ°è©•ä¼°å„ç¨®è§€é»çš„å„ªåŠ£ã€‚",
            'empathetic': "æ¡ç”¨å»ºè¨­æ€§å°è©±ç­–ç•¥ï¼šç†è§£å°æ–¹çš„åˆç†é—œåˆ‡ï¼Œå°‹æ‰¾å…±åŒé»ï¼Œæå‡ºå…¼é¡§å„æ–¹åˆ©ç›Šçš„è§£æ±ºæ–¹æ¡ˆã€‚"
        }
        
        # è¾¯è«–é¢¨æ ¼æç¤º
        if is_first_round:
            debate_style = """
é€™æ˜¯é—œæ–¼å…¬å…±è­°é¡Œçš„ç†æ€§è¾¯è«–ï¼Œä½ éœ€è¦ï¼š
- æ¸…æ¥šé™³è¿°ä½ å°è­°é¡Œçš„ç«‹å ´ï¼ˆæ”¯æŒæˆ–åå°ï¼‰
- æå‡ºä½ çš„æ ¸å¿ƒè«–é»å’Œç†ç”±
- ä½¿ç”¨äº‹å¯¦å’Œé‚è¼¯æ”¯æŒä½ çš„è§€é»
- å±•ç¾æ‰¹åˆ¤æ€§æ€ç¶­èƒ½åŠ›
- ä¸è¦æåŠå…¶ä»–åƒèˆ‡è€…ï¼ˆå› ç‚ºä½ é‚„æ²’è½éä»–å€‘çš„ç™¼è¨€ï¼‰
"""
            round_instruction = """
ç¬¬ä¸€å›åˆè¦æ±‚ï¼š
1. æ˜ç¢ºè¡¨æ˜ä½ æ˜¯æ”¯æŒé‚„æ˜¯åå°è©²è­°é¡Œ
2. æå‡º3-4å€‹æ ¸å¿ƒè«–é»
3. ä½¿ç”¨å…·é«”äº‹å¯¦å’Œæ•¸æ“šæ”¯æŒä½ çš„ç«‹å ´
4. å¾ç¤¾æœƒå½±éŸ¿ã€ç¶“æ¿Ÿæ•ˆç›Šã€é•·é ç™¼å±•ç­‰è§’åº¦åˆ†æ
5. ä¸è¦æåŠå…¶ä»–åƒèˆ‡è€…
6. å­—æ•¸è¦æ±‚ï¼š200-250å­—
7. å¿…é ˆå®Œæˆå®Œæ•´çš„è«–è¿°ï¼Œä¸è¦ä¸­é€”æˆªæ–·
"""
        else:
            # æ ¹æ“šå›åˆæ•¸èª¿æ•´ç­–ç•¥
            round_num = len([h for h in history if agent_id in h]) + 1
            if round_num >= 3 and agent_state.conviction < 0.4:
                debate_style = """
ç¶“éæ·±å…¥è¨è«–ï¼Œä½ é–‹å§‹é‡æ–°æ€è€ƒé€™å€‹è­°é¡Œï¼š
- æ‰¿èªå°æ–¹æŸäº›è«–é»ç¢ºå¯¦æœ‰é“ç†
- åæ€è‡ªå·±ç«‹å ´çš„å±€é™æ€§
- å˜—è©¦å°‹æ‰¾ä¸­é–“ç«‹å ´æˆ–å¦¥å”æ–¹æ¡ˆ
- å±•ç¾é–‹æ”¾å’Œç†æ€§çš„æ…‹åº¦
"""
            elif round_num >= 4:
                debate_style = """
è¾¯è«–é€²å…¥ç¸½çµéšæ®µï¼Œä½ éœ€è¦ï¼š
- æ•´åˆå„æ–¹è§€é»ï¼Œæå‡ºå…¨é¢çš„åˆ†æ
- æŒ‡å‡ºæ ¸å¿ƒåˆ†æ­§é»å’Œå…±è­˜
- æå‡ºå¯è¡Œçš„è§£æ±ºæ–¹æ¡ˆæˆ–å»ºè­°
- ç‚ºé€™å€‹å…¬å…±è­°é¡Œçš„è¨è«–åšå‡ºå»ºè¨­æ€§è²¢ç»
"""
            else:
                debate_style = """
é€™æ˜¯ä¸€å ´æ·±å…¥çš„å…¬å…±è­°é¡Œè¾¯è«–ï¼Œä½ éœ€è¦ï¼š
- å›æ‡‰å°æ–¹çš„å…·é«”è«–é»
- æä¾›æ›´å¤šè­‰æ“šå’Œåˆ†æ
- å¾ä¸åŒè§’åº¦æ·±åŒ–ä½ çš„è«–è­‰
- ä¿æŒç†æ€§å’Œå°ˆæ¥­çš„è¨è«–æ°›åœ
"""
            
            # é¿å…é‡è¤‡çš„æŒ‡å°
            avoid_repetition = ""
            if used_arguments:
                avoid_repetition = f"""
æ³¨æ„ï¼šä½ å·²ç¶“è¨è«–éä»¥ä¸‹æ–¹é¢ï¼Œè«‹æ¢è¨æ–°çš„è§’åº¦ï¼š
{chr(10).join(f"- {arg}" for arg in used_arguments[:3])}

å˜—è©¦å¾å…¶ä»–ç¶­åº¦åˆ†æï¼Œå¦‚ï¼šç¤¾æœƒå…¬å¹³ã€å¯æŒçºŒç™¼å±•ã€åœ‹éš›æ¯”è¼ƒã€æ­·å²ç¶“é©—ç­‰ã€‚
"""
            
            round_instruction = f"""
è¾¯è«–è¦æ±‚ï¼š
1. å›æ‡‰å°æ–¹çš„æ ¸å¿ƒè«–é»ï¼ŒæŒ‡å‡ºé‚è¼¯æ¼æ´æˆ–æä¾›åè­‰
2. æ·±åŒ–ä½ çš„è«–è­‰ï¼Œæä¾›æ–°çš„è¦–è§’å’Œè­‰æ“š
3. ä½¿ç”¨ã€Œä½†æ˜¯ã€ã€ã€Œç„¶è€Œã€ã€ã€Œå¦ä¸€æ–¹é¢ã€ç­‰é€£æ¥è©
4. å¼•ç”¨å…·é«”æ¡ˆä¾‹æˆ–æ•¸æ“šæ™‚ä½¿ç”¨ [CITE] æ¨™è¨˜
5. ä¿æŒå®¢è§€ç†æ€§ï¼Œé¿å…äººèº«æ”»æ“Š
6. å¦‚æœè¢«èªªæœï¼Œå¯ä»¥é©ç•¶èª¿æ•´ç«‹å ´
7. é¿å…é‡è¤‡å·²ç¶“è¨è«–éçš„å…§å®¹
8. å­—æ•¸è¦æ±‚ï¼š200-250å­—
9. å¿…é ˆå®Œæˆå®Œæ•´çš„è«–è¿°ï¼Œç¢ºä¿è«–é»æœ‰é ­æœ‰å°¾

{avoid_repetition}
"""
        
        # æ§‹å»ºæç¤º
        if is_first_round:
            return f"""ä½ æ­£åœ¨åƒèˆ‡ä¸€å ´é—œæ–¼ã€Œ{topic}ã€çš„å…¬å…±è­°é¡Œè¾¯è«–ã€‚

{debate_style}

ä½ çš„è§’è‰²è¨­å®šï¼š
- ç«‹å ´å‚¾å‘ï¼š{agent_state.current_stance:.2f} (æ­£å€¼å‚¾å‘æ”¯æŒï¼Œè² å€¼å‚¾å‘åå°)
- å …å®šç¨‹åº¦ï¼š{agent_state.conviction:.2f} (è¶Šé«˜è¶Šä¸æ˜“æ”¹è®Šç«‹å ´)
- è«–è­‰é¢¨æ ¼ï¼š{strategy}

å¯ç”¨è«–æ“šï¼š
{evidence}

{round_instruction}

è«‹ç™¼è¡¨ä½ çš„è§€é»ï¼š"""
        else:
            return f"""ä½ æ­£åœ¨åƒèˆ‡ä¸€å ´é—œæ–¼ã€Œ{topic}ã€çš„å…¬å…±è­°é¡Œè¾¯è«–ã€‚

{debate_style}

ä½ çš„ç•¶å‰ç‹€æ…‹ï¼š
- ç«‹å ´å‚¾å‘ï¼š{agent_state.current_stance:.2f} (æ­£å€¼å‚¾å‘æ”¯æŒï¼Œè² å€¼å‚¾å‘åå°)
- å …å®šç¨‹åº¦ï¼š{agent_state.conviction:.2f} (è¶Šé«˜è¶Šä¸æ˜“æ”¹è®Šç«‹å ´)
- è«–è­‰é¢¨æ ¼ï¼š{strategy}

è¨è«–è¨˜éŒ„ï¼š
{history_text}

å¯ç”¨è«–æ“šï¼š
{evidence}

å…¶ä»–åƒèˆ‡è€…ç‹€æ…‹ï¼š{target_info}

è«–è­‰ç­–ç•¥ï¼š{strategy_guidance.get(strategy, '')}

{round_instruction}

è«‹ç™¼è¡¨ä½ çš„è§€é»ï¼š"""
    
    def _extract_used_arguments(self, history: List[str], agent_id: str) -> List[str]:
        """æå–å·²ä½¿ç”¨çš„è«–é»é—œéµè©"""
        used_arguments = []
        
        # ç°¡å–®çš„é—œéµè©æå–
        keywords = ['ç¶“æ¿Ÿ', 'å¤±æ¥­ç‡', 'å¤–äº¤', 'è²¿æ˜“', 'ç’°å¢ƒ', 'æ°£å€™', 'å®‰å…¨', 'æ”¿ç­–', 
                   'è²¡æ”¿', 'èµ¤å­—', 'åœ‹éš›', 'é ˜å°', 'ç¤¾æœƒ', 'åˆ†è£‚']
        
        for turn in history:
            if agent_id in turn:
                for keyword in keywords:
                    if keyword in turn and keyword not in used_arguments:
                        used_arguments.append(keyword)
        
        return used_arguments
    
    def _evaluate_response(self, response: str, target_agents: List[str]) -> Dict:
        """è©•ä¼°å›è¦†çš„èªªæœåŠ›å’Œæ”»æ“Šæ€§"""
        
        # ä¸­è‹±æ–‡é—œéµè©è©•ä¼°
        persuasion_indicators = [
            'however', 'consider', 'understand', 'perspective', 'common',
            'ä½†æ˜¯', 'è€ƒæ…®', 'ç†è§£', 'è§€é»', 'å…±åŒ', 'èªåŒ', 'åŒæ„'
        ]
        attack_indicators = [
            'wrong', 'flawed', 'mistake', 'ignore', 'fail',
            'éŒ¯èª¤', 'ç¼ºé™·', 'è¬¬èª¤', 'å¿½è¦–', 'å¤±æ•—', 'è’è¬¬', 'ä¸åˆç†'
        ]
        evidence_indicators = [
            '[CITE]', 'study', 'research', 'data', 'evidence',
            'ç ”ç©¶', 'æ•¸æ“š', 'è­‰æ“š', 'äº‹å¯¦', 'çµ±è¨ˆ', 'å ±å‘Š', 'èª¿æŸ¥'
        ]
        
        # è¨ˆç®—åˆ†æ•¸ï¼ˆä½¿ç”¨æ›´æ•æ„Ÿçš„è¨ˆç®—æ–¹å¼ï¼‰
        response_lower = response.lower()
        persuasion_count = sum(1 for indicator in persuasion_indicators if indicator in response_lower)
        attack_count = sum(1 for indicator in attack_indicators if indicator in response_lower)
        evidence_count = sum(1 for indicator in evidence_indicators if indicator in response_lower)
        
        # èª¿æ•´åˆ†æ•¸è¨ˆç®—ï¼Œä½¿å…¶æ›´å®¹æ˜“ç²å¾—éé›¶åˆ†æ•¸
        persuasion_score = min(1.0, persuasion_count * 0.3)
        attack_score = min(1.0, attack_count * 0.4)
        evidence_score = min(1.0, evidence_count * 0.35)
        
        # é•·åº¦åˆ†æ•¸
        word_count = len(response.split())
        length_score = min(1.0, word_count / 80)
        
        return {
            'persuasion_score': persuasion_score,
            'attack_score': attack_score,
            'evidence_score': evidence_score,
            'length_score': length_score
        }
    
    async def run_debate_round(self, round_number: int, topic: str, 
                             agent_order: List[str]) -> DebateRound:
        """åŸ·è¡Œä¸€è¼ªè¾¯è«–"""
        
        print(f"\nğŸ­ é–‹å§‹ç¬¬ {round_number} è¼ªè¾¯è«–")
        print(f"ä¸»é¡Œ: {topic}")
        print(f"ç™¼è¨€é †åº: {' â†’ '.join(agent_order)}")
        
        # ç²å–æ‰€æœ‰ä¹‹å‰çš„æ­·å²è¨˜éŒ„
        all_history = []
        for past_round in self.debate_history:
            for response in past_round.history:
                all_history.append(f"{response['agent_id']}: {response['content']}")
        
        # ç•¶å‰å›åˆçš„å›æ‡‰
        round_responses = []
        
        for i, agent_id in enumerate(agent_order):
            print(f"\n--- Agent {agent_id} ç™¼è¨€ ---")
            
            # ç¢ºå®šç›®æ¨™å°æ‰‹
            other_agents = [aid for aid in agent_order if aid != agent_id]
            
            # çµ„åˆæ­·å²ï¼šä¹‹å‰æ‰€æœ‰å›åˆ + ç•¶å‰å›åˆå·²ç™¼è¨€
            current_history = all_history + [f"{r['agent_id']}: {r['content']}" for r in round_responses]
            
            # ç”Ÿæˆå›è¦†
            response = await self.generate_response(
                agent_id, topic, 
                current_history, 
                other_agents
            )
            
            # è©•ä¼°æ•ˆæœ
            effects = self._evaluate_response(response, other_agents)
            
            # è¨˜éŒ„å›è¦†
            round_responses.append({
                'agent_id': agent_id,
                'content': response,
                'effects': effects,
                'timestamp': time.time()
            })
            
            print(f"Agent {agent_id}: {response[:100]}...")
            print(f"æ•ˆæœè©•ä¼°: èªªæœ{effects['persuasion_score']:.2f}, æ”»æ“Š{effects['attack_score']:.2f}")
            
            # æ›´æ–°å…¶ä»– Agent çš„ç‹€æ…‹
            for target_id in other_agents:
                if target_id in self.agent_states:
                    self.agent_states[target_id].update_stance(
                        effects['persuasion_score'],
                        effects['attack_score']
                    )
        
        # å‰µå»ºå›åˆè¨˜éŒ„
        debate_round = DebateRound(
            round_number=round_number,
            topic=topic,
            agent_states=dict(self.agent_states),
            history=round_responses
        )
        
        self.debate_history.append(debate_round)
        
        # é¡¯ç¤ºå›åˆçµæœ
        self._display_round_summary(debate_round)
        
        return debate_round
    
    def _display_round_summary(self, debate_round: DebateRound):
        """é¡¯ç¤ºå›åˆç¸½çµ"""
        print(f"\nğŸ“Š ç¬¬ {debate_round.round_number} è¼ªç¸½çµ")
        print("=" * 50)
        
        for agent_id, state in debate_round.agent_states.items():
            print(f"Agent {agent_id}:")
            print(f"  ç«‹å ´: {state.current_stance:+.2f} | ä¿¡å¿µ: {state.conviction:.2f}")
            if state.persuasion_history:
                avg_persuasion = sum(state.persuasion_history[-3:]) / min(3, len(state.persuasion_history))
                print(f"  è¿‘æœŸè¢«èªªæœåº¦: {avg_persuasion:.2f}")
    
    def get_debate_summary(self) -> Dict:
        """ç²å–è¾¯è«–ç¸½çµå’Œå‹è² åˆ¤å®š"""
        if not self.debate_history:
            return {"message": "å°šæœªé–‹å§‹è¾¯è«–"}
        
        # çµ±è¨ˆæŠ•é™æƒ…æ³
        surrendered_agents = [aid for aid, state in self.agent_states.items() 
                            if state.has_surrendered]
        
        # è¨ˆç®—æ¯å€‹ Agent çš„ç¶œåˆå¾—åˆ†
        agent_scores = {}
        for agent_id, state in self.agent_states.items():
            # åŸºç¤åˆ†æ•¸
            score = 0
            
            # ç«‹å ´å …å®šåº¦å¾—åˆ†ï¼ˆç«‹å ´è¶Šæ¥µç«¯ä¸”ä¿¡å¿µè¶Šå¼·å¾—åˆ†è¶Šé«˜ï¼‰
            stance_score = abs(state.current_stance) * state.conviction * 30
            score += stance_score
            
            # èªªæœä»–äººå¾—åˆ†ï¼ˆæ ¹æ“šå…¶ä»–äººçš„æŠ•é™å’Œç«‹å ´æ”¹è®Šï¼‰
            persuasion_score = 0
            for other_id, other_state in self.agent_states.items():
                if other_id != agent_id:
                    if other_state.has_surrendered:
                        persuasion_score += 20
                    # è¨ˆç®—å°å…¶ä»–äººçš„å½±éŸ¿
                    if len(other_state.persuasion_history) > 0:
                        avg_persuasion = sum(other_state.persuasion_history) / len(other_state.persuasion_history)
                        persuasion_score += avg_persuasion * 10
            score += persuasion_score
            
            # æŠ—å£“èƒ½åŠ›å¾—åˆ†ï¼ˆè¢«æ”»æ“Šä½†ä»ä¿æŒç«‹å ´ï¼‰
            if len(state.attack_history) > 0:
                avg_attack = sum(state.attack_history) / len(state.attack_history)
                resistance_score = (1 - avg_attack) * state.conviction * 20
                score += resistance_score
            
            # æŠ•é™æ‰£åˆ†
            if state.has_surrendered:
                score -= 50
            
            agent_scores[agent_id] = score
        
        # åˆ¤å®šç²å‹è€…
        winner = max(agent_scores.keys(), key=lambda x: agent_scores[x])
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        summary = {
            "total_rounds": len(self.debate_history),
            "winner": winner,
            "scores": agent_scores,
            "surrendered_agents": surrendered_agents,
            "final_states": {},
            "verdict": ""
        }
        
        # æ·»åŠ æœ€çµ‚ç‹€æ…‹
        for aid, state in self.agent_states.items():
            summary["final_states"][aid] = {
                "stance": state.current_stance,
                "conviction": state.conviction,
                "has_surrendered": state.has_surrendered,
                "final_position": "æ”¯æŒ" if state.current_stance > 0 else "åå°"
            }
        
        # ç”Ÿæˆè£æ±ºè©
        if len(surrendered_agents) > 0:
            summary["verdict"] = f"ğŸ† {winner} ç²å¾—å£“å€’æ€§å‹åˆ©ï¼æˆåŠŸèªªæœ {', '.join(surrendered_agents)} æŠ•é™ã€‚"
        else:
            score_diff = agent_scores[winner] - sorted(agent_scores.values())[-2]
            if score_diff > 30:
                summary["verdict"] = f"ğŸ† {winner} ä»¥æ˜é¡¯å„ªå‹¢ç²å‹ï¼å±•ç¾äº†å“è¶Šçš„è¾¯è«–æŠ€å·§ã€‚"
            else:
                summary["verdict"] = f"ğŸ† {winner} éšªå‹ï¼é€™æ˜¯ä¸€å ´å‹¢å‡åŠ›æ•µçš„ç²¾å½©è¾¯è«–ã€‚"
        
        return summary

# ä¾¿åˆ©å‡½æ•¸
def create_parallel_orchestrator():
    """å‰µå»ºå¹³è¡Œå”èª¿å™¨"""
    return ParallelOrchestrator() 