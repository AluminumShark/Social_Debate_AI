"""
RL ç­–ç•¥ç¶²è·¯è¨“ç·´å™¨
æ”¯æ´ CUDA åŠ é€Ÿå’Œæ··åˆç²¾åº¦è¨“ç·´
"""

import pandas as pd
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer,
    EarlyStoppingCallback
)
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import json
import time
from typing import Dict, Optional

class RLTrainer:
    """RL ç­–ç•¥ç¶²è·¯è¨“ç·´å™¨"""
    
    def __init__(self, 
                 data_path: str = "data/rl/rl_pairs.csv",
                 model_name: str = "distilbert-base-uncased",
                 output_dir: str = "data/models/policy",
                 max_length: int = 512):
        
        self.data_path = Path(data_path)
        self.model_name = model_name
        self.output_dir = Path(output_dir)
        self.max_length = max_length
        
        # è¨­å‚™æª¢æ¸¬
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ RL è¨“ç·´å™¨åˆå§‹åŒ–")
        print(f"  æ•¸æ“šè·¯å¾‘: {self.data_path}")
        print(f"  æ¨¡å‹: {self.model_name}")
        print(f"  è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        print(f"  è¨­å‚™: {self.device}")
        
        if torch.cuda.is_available():
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    def load_data(self) -> pd.DataFrame:
        """è¼‰å…¥è¨“ç·´æ•¸æ“š"""
        print(f"ğŸ“‚ è¼‰å…¥æ•¸æ“š: {self.data_path}")
        
        if not self.data_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶: {self.data_path}")
        
        df = pd.read_csv(self.data_path)
        print(f"  æ•¸æ“šç¸½æ•¸: {len(df)} ç­†")
        print(f"  åˆ†æ•¸ç¯„åœ: {df['score'].min():.3f} ~ {df['score'].max():.3f}")
        print(f"  å¹³å‡åˆ†æ•¸: {df['score'].mean():.3f}")
        
        # æª¢æŸ¥æ•¸æ“šå“è³ª
        null_count = df.isnull().sum().sum()
        if null_count > 0:
            print(f"âš ï¸  ç™¼ç¾ {null_count} å€‹ç©ºå€¼ï¼Œå°‡è‡ªå‹•è™•ç†")
            df = df.dropna()
        
        # æª¢æŸ¥æ–‡æœ¬é•·åº¦
        text_lengths = df['text'].str.len()
        print(f"  æ–‡æœ¬é•·åº¦: å¹³å‡ {text_lengths.mean():.0f}, æœ€å¤§ {text_lengths.max()}")
        
        return df
    
    def prepare_datasets(self, df: pd.DataFrame) -> Dict:
        """æº–å‚™è¨“ç·´å’Œé©—è­‰æ•¸æ“šé›†"""
        print("ğŸ”„ æº–å‚™æ•¸æ“šé›†...")
        
        # åˆ†å‰²æ•¸æ“š
        dataset = Dataset.from_pandas(df[['text', 'score']])
        split_dataset = dataset.train_test_split(test_size=0.2, seed=42)
        
        print(f"  è¨“ç·´é›†: {len(split_dataset['train'])} ç­†")
        print(f"  é©—è­‰é›†: {len(split_dataset['test'])} ç­†")
        
        return split_dataset
    
    def tokenize_function(self, examples, tokenizer):
        """Tokenization å‡½æ•¸"""
        return tokenizer(
            examples['text'],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors=None
        )
    
    def compute_metrics(self, eval_pred):
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        predictions, labels = eval_pred
        predictions = predictions.flatten()
        
        mse = mean_squared_error(labels, predictions)
        mae = mean_absolute_error(labels, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(labels, predictions)
        
        return {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'r2': r2
        }
    
    def get_training_args(self) -> TrainingArguments:
        """ç²å–è¨“ç·´åƒæ•¸"""
        # æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´æ‰¹æ¬¡å¤§å°
        if torch.cuda.is_available():
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory_gb >= 20:  # RTX 3090+ 
                train_batch_size = 32
                eval_batch_size = 64
            elif gpu_memory_gb >= 10:  # RTX 3080
                train_batch_size = 24
                eval_batch_size = 32
            else:  # è¼ƒå°çš„ GPU
                train_batch_size = 16
                eval_batch_size = 16
            
            print(f"  GPU è¨˜æ†¶é«” {gpu_memory_gb:.1f}GBï¼Œæ‰¹æ¬¡å¤§å°: è¨“ç·´={train_batch_size}, è©•ä¼°={eval_batch_size}")
        else:
            train_batch_size = 8
            eval_batch_size = 8
            print(f"  CPU æ¨¡å¼ï¼Œæ‰¹æ¬¡å¤§å°: {train_batch_size}")
        
        # è‡¨æ™‚ç›®éŒ„
        temp_dir = Path("data/_tmp/rl_training")
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        return TrainingArguments(
            output_dir=str(temp_dir),
            learning_rate=5e-5,
            num_train_epochs=3,
            per_device_train_batch_size=train_batch_size,
            per_device_eval_batch_size=eval_batch_size,
            eval_strategy='epoch',
            save_strategy='epoch',
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model='eval_mse',
            greater_is_better=False,
            logging_steps=50,
            logging_dir=str(temp_dir / "logs"),
            report_to=[],
            seed=42,
            dataloader_pin_memory=False,
            fp16=torch.cuda.is_available(),  # æ··åˆç²¾åº¦è¨“ç·´
            dataloader_num_workers=0,  # é¿å…å¤šé€²ç¨‹å•é¡Œ
            remove_unused_columns=False,
        )
    
    def train(self) -> Dict:
        """åŸ·è¡Œè¨“ç·´"""
        print("ğŸš€ é–‹å§‹è¨“ç·´...")
        
        # 1. è¼‰å…¥æ•¸æ“š
        df = self.load_data()
        
        # 2. æº–å‚™æ•¸æ“šé›†
        datasets = self.prepare_datasets(df)
        
        # 3. è¼‰å…¥ tokenizer å’Œæ¨¡å‹
        print(f"ğŸ“¦ è¼‰å…¥æ¨¡å‹: {self.model_name}")
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=1,
            problem_type='regression'
        )
        
        # 4. Tokenization
        print("ğŸ”¤ é€²è¡Œ tokenization...")
        tokenized_datasets = datasets.map(
            lambda x: self.tokenize_function(x, tokenizer),
            batched=True,
            remove_columns=['text']
        )
        
        # é‡å‘½åæ¨™ç±¤åˆ—
        tokenized_datasets = tokenized_datasets.rename_column('score', 'labels')
        
        # è¨­ç½®æ ¼å¼
        tokenized_datasets.set_format(
            type='torch', 
            columns=['input_ids', 'attention_mask', 'labels']
        )
        
        # 5. ç§»å‹•æ¨¡å‹åˆ° GPU
        model = model.to(self.device)
        print(f"ğŸ“± æ¨¡å‹å·²ç§»è‡³: {next(model.parameters()).device}")
        
        # 6. è¨­ç½®è¨“ç·´åƒæ•¸
        training_args = self.get_training_args()
        
        # 7. å‰µå»º Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets['train'],
            eval_dataset=tokenized_datasets['test'],
            tokenizer=tokenizer,
            compute_metrics=self.compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
        )
        
        # 8. é–‹å§‹è¨“ç·´
        print("ğŸ¯ é–‹å§‹è¨“ç·´...")
        start_time = time.time()
        
        if torch.cuda.is_available():
            print(f"è¨“ç·´å‰ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
        
        train_result = trainer.train()
        
        training_time = time.time() - start_time
        print(f"â±ï¸  è¨“ç·´å®Œæˆï¼Œè€—æ™‚: {training_time:.2f} ç§’")
        
        if torch.cuda.is_available():
            print(f"è¨“ç·´å¾Œ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
            print(f"GPU è¨˜æ†¶é«”å³°å€¼: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB")
        
        # 9. æœ€çµ‚è©•ä¼°
        print("ğŸ“Š æœ€çµ‚è©•ä¼°...")
        eval_results = trainer.evaluate()
        
        print("è©•ä¼°çµæœ:")
        for key, value in eval_results.items():
            print(f"  {key}: {value:.4f}")
        
        # 10. ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {self.output_dir}")
        model.save_pretrained(str(self.output_dir))
        tokenizer.save_pretrained(str(self.output_dir))
        
        # 11. ä¿å­˜è¨“ç·´è¨˜éŒ„
        training_log = {
            'model_name': self.model_name,
            'training_time': training_time,
            'train_samples': len(tokenized_datasets['train']),
            'eval_samples': len(tokenized_datasets['test']),
            'final_eval_results': eval_results,
            'train_results': {
                'train_loss': train_result.training_loss,
                'train_runtime': train_result.metrics['train_runtime'],
                'train_samples_per_second': train_result.metrics['train_samples_per_second'],
            }
        }
        
        log_path = self.output_dir / "training_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(training_log, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ è¨“ç·´è¨˜éŒ„ä¿å­˜åˆ°: {log_path}")
        
        return {
            'eval_results': eval_results,
            'training_time': training_time,
            'model_path': self.output_dir
        }

def main():
    """ä¸»å‡½æ•¸"""
    trainer = RLTrainer()
    
    try:
        results = trainer.train()
        
        print("\nğŸ‰ è¨“ç·´å®Œæˆï¼")
        print("ğŸ“Š æœ€çµ‚çµæœ:")
        for key, value in results['eval_results'].items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        print(f"  è¨“ç·´æ™‚é–“: {results['training_time']:.2f} ç§’")
        print(f"  æ¨¡å‹è·¯å¾‘: {results['model_path']}")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        raise

if __name__ == "__main__":
    main() 