"""
PPO è¨“ç·´ä¸»ç¨‹å¼
ä½¿ç”¨çœŸæ­£çš„å¼·åŒ–å­¸ç¿’è¨“ç·´è¾¯è«–ç­–ç•¥
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.rl.ppo_trainer import PPOTrainer, DebateEnvironment
from src.utils.config_loader import ConfigLoader
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import matplotlib.pyplot as plt
import argparse
import json

def plot_training_curve(rewards, save_path):
    """ç¹ªè£½è¨“ç·´æ›²ç·š"""
    plt.figure(figsize=(10, 6))
    
    # è¨ˆç®—ç§»å‹•å¹³å‡
    window = 100
    if len(rewards) >= window:
        avg_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(rewards)), avg_rewards, label=f'{window}-episode average')
    
    plt.plot(rewards, alpha=0.3, label='Episode reward')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('PPO Training Progress')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()
    print(f"è¨“ç·´æ›²ç·šä¿å­˜åˆ°: {save_path}")

def main():
    parser = argparse.ArgumentParser(description="è¨“ç·´ PPO è¾¯è«–ç­–ç•¥")
    parser.add_argument("--episodes", type=int, default=1000, help="è¨“ç·´å›åˆæ•¸")
    parser.add_argument("--lr", type=float, default=3e-4, help="å­¸ç¿’ç‡")
    parser.add_argument("--gamma", type=float, default=0.99, help="æŠ˜æ‰£å› å­")
    parser.add_argument("--eps_clip", type=float, default=0.2, help="PPO epsilon clipping")
    parser.add_argument("--k_epochs", type=int, default=4, help="PPO æ›´æ–°æ¬¡æ•¸")
    parser.add_argument("--output_dir", type=str, default="data/models/ppo", help="è¼¸å‡ºç›®éŒ„")
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ PPO è¾¯è«–ç­–ç•¥è¨“ç·´")
    print("=" * 60)
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¼‰å…¥é…ç½®
    config = ConfigLoader.load("rl")
    
    # åˆå§‹åŒ– tokenizer å’Œ encoderï¼ˆç”¨æ–¼ç’°å¢ƒï¼‰
    print("è¼‰å…¥èªè¨€æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
    encoder = AutoModel.from_pretrained('distilbert-base-uncased')
    encoder.eval()
    
    # å‰µå»ºç’°å¢ƒ
    env = DebateEnvironment(tokenizer, encoder)
    
    # å‰µå»º PPO è¨“ç·´å™¨
    trainer = PPOTrainer(
        state_dim=768,
        hidden_dim=256,
        num_actions=4,
        learning_rate=args.lr,
        gamma=args.gamma,
        eps_clip=args.eps_clip,
        k_epochs=args.k_epochs
    )
    
    print(f"\nè¨“ç·´åƒæ•¸:")
    print(f"  è¨“ç·´å›åˆ: {args.episodes}")
    print(f"  å­¸ç¿’ç‡: {args.lr}")
    print(f"  Gamma: {args.gamma}")
    print(f"  Epsilon clip: {args.eps_clip}")
    print(f"  K epochs: {args.k_epochs}")
    print("-" * 60)
    
    # è¨“ç·´
    print("\né–‹å§‹è¨“ç·´...")
    episode_rewards = trainer.train(env, num_episodes=args.episodes)
    
    # ä¿å­˜æ¨¡å‹
    model_path = output_dir / "ppo_debate_policy.pt"
    trainer.save(str(model_path))
    
    # ä¿å­˜è¨“ç·´è¨˜éŒ„
    training_log = {
        'episodes': args.episodes,
        'final_avg_reward': np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards),
        'max_reward': max(episode_rewards),
        'min_reward': min(episode_rewards),
        'hyperparameters': {
            'learning_rate': args.lr,
            'gamma': args.gamma,
            'eps_clip': args.eps_clip,
            'k_epochs': args.k_epochs
        }
    }
    
    log_path = output_dir / "training_log.json"
    with open(log_path, 'w') as f:
        json.dump(training_log, f, indent=2)
    
    # ç¹ªè£½è¨“ç·´æ›²ç·š
    plot_path = output_dir / "training_curve.png"
    plot_training_curve(episode_rewards, plot_path)
    
    print(f"\nâœ… è¨“ç·´å®Œæˆï¼")
    print(f"  æœ€çµ‚å¹³å‡çå‹µ: {training_log['final_avg_reward']:.3f}")
    print(f"  æœ€é«˜çå‹µ: {training_log['max_reward']:.3f}")
    print(f"  æœ€ä½çå‹µ: {training_log['min_reward']:.3f}")

if __name__ == "__main__":
    main() 