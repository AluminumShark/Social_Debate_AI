"""
RL Policy Network for Debate Strategy
åŒ…å« snippet é¸æ“‡ã€ç­–ç•¥æ±ºç­–å’Œå“è³ªè©•ä¼°åŠŸèƒ½
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import json
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class DebatePolicy(nn.Module):
    """è¾¯è«–ç­–ç•¥ç¶²è·¯"""
    
    def __init__(self, hidden_size=256, social_dim=128):
        super().__init__()
        
        # Text encoder (ä½¿ç”¨é è¨“ç·´æ¨¡å‹çš„ç‰¹å¾µ)
        self.text_encoder = nn.Linear(768, hidden_size)  # DistilBERT è¼¸å‡ºç¶­åº¦
        
        # Social context encoder
        self.social_encoder = nn.Linear(social_dim, hidden_size // 2)
        
        # Fusion layer
        self.fusion = nn.Linear(hidden_size + hidden_size // 2, hidden_size)
        
        # Strategy selection head
        self.strategy_head = nn.Linear(hidden_size, 4)  # 4ç¨®ç­–ç•¥
        
        # Snippet ranking head
        self.ranking_head = nn.Linear(hidden_size * 2, 1)  # query + snippet
        
        # Quality prediction head
        self.quality_head = nn.Linear(hidden_size, 1)
        
        self.dropout = nn.Dropout(0.1)
        self.activation = nn.ReLU()
        
    def forward(self, text_features, social_features=None, snippet_features=None):
        """å‰å‘å‚³æ’­"""
        # Encode text
        text_enc = self.activation(self.text_encoder(text_features))
        
        # Encode social context if available
        if social_features is not None:
            social_enc = self.activation(self.social_encoder(social_features))
            combined = torch.cat([text_enc, social_enc], dim=-1)
        else:
            # Use zero padding for social features
            social_enc = torch.zeros(text_enc.size(0), text_enc.size(1) // 2, device=text_enc.device)
            combined = torch.cat([text_enc, social_enc], dim=-1)
        
        # Fusion
        fused = self.activation(self.fusion(combined))
        fused = self.dropout(fused)
        
        outputs = {}
        
        # Strategy selection
        outputs['strategy_logits'] = self.strategy_head(fused)
        
        # Quality prediction
        outputs['quality_score'] = self.quality_head(fused)
        
        # Snippet ranking if snippet features provided
        if snippet_features is not None:
            snippet_enc = self.activation(self.text_encoder(snippet_features))
            ranking_input = torch.cat([fused, snippet_enc], dim=-1)
            outputs['ranking_score'] = self.ranking_head(ranking_input)
        
        return outputs

class PolicyNetwork:
    """ç­–ç•¥ç¶²è·¯ç®¡ç†å™¨"""
    
    def __init__(self, model_path: str = None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è¼‰å…¥é è¨“ç·´çš„å“è³ªè©•ä¼°æ¨¡å‹
        self.quality_model_path = Path("data/models/policy")
        self.load_quality_model()
        
        # è¼‰å…¥ç­–ç•¥ç¶²è·¯
        self.policy_model = DebatePolicy()
        if model_path and Path(model_path).exists():
            self.load_policy_model(model_path)
        
        self.policy_model.to(self.device)
        self.policy_model.eval()
        
        # ç­–ç•¥æ˜ å°„
        self.strategies = {
            0: 'aggressive',     # ç©æ¥µæ”»æ“Šå‹
            1: 'defensive',      # é˜²ç¦¦åé§å‹  
            2: 'analytical',     # åˆ†æè«–è­‰å‹
            3: 'empathetic'      # åŒç†èªªæœå‹
        }
        
    def load_quality_model(self):
        """è¼‰å…¥å“è³ªè©•ä¼°æ¨¡å‹"""
        try:
            if self.quality_model_path.exists():
                self.tokenizer = AutoTokenizer.from_pretrained(str(self.quality_model_path))
                # ä¿®å¾© meta tensor éŒ¯èª¤ï¼šå…ˆè¼‰å…¥åˆ° CPUï¼Œå†ç§»å‹•åˆ°ç›®æ¨™è¨­å‚™
                self.quality_model = AutoModelForSequenceClassification.from_pretrained(
                    str(self.quality_model_path),
                    torch_dtype=torch.float32,  # æŒ‡å®šæ•¸æ“šé¡å‹
                    low_cpu_mem_usage=False     # é—œé–‰ä½å…§å­˜æ¨¡å¼
                )
                # ç¢ºä¿æ¨¡å‹åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
                self.quality_model = self.quality_model.to(self.device)
                self.quality_model.eval()
                print(f"âœ… è¼‰å…¥å“è³ªè©•ä¼°æ¨¡å‹: {self.quality_model_path}")
            else:
                print("âš ï¸ å“è³ªè©•ä¼°æ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é è¨­è©•åˆ†")
                self.tokenizer = None
                self.quality_model = None
        except Exception as e:
            print(f"âŒ è¼‰å…¥å“è³ªè©•ä¼°æ¨¡å‹å¤±æ•—: {e}")
            self.tokenizer = None
            self.quality_model = None
    
    def load_policy_model(self, model_path: str):
        """è¼‰å…¥ç­–ç•¥æ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            self.policy_model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… è¼‰å…¥ç­–ç•¥æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥ç­–ç•¥æ¨¡å‹å¤±æ•—ï¼Œä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–: {e}")
    
    def encode_text(self, text: str) -> torch.Tensor:
        """å°‡æ–‡æœ¬ç·¨ç¢¼ç‚ºç‰¹å¾µå‘é‡"""
        if self.tokenizer and self.quality_model:
            try:
                inputs = self.tokenizer(text, 
                                      truncation=True, 
                                      padding=True, 
                                      max_length=512,
                                      return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    # ä½¿ç”¨å“è³ªæ¨¡å‹çš„éš±è—å±¤ä½œç‚ºç‰¹å¾µ
                    outputs = self.quality_model(**inputs, output_hidden_states=True)
                    # å–æœ€å¾Œä¸€å±¤çš„ [CLS] token
                    features = outputs.hidden_states[-1][:, 0, :]  # [batch_size, hidden_size]
                
                return features
            except Exception as e:
                print(f"âŒ æ–‡æœ¬ç·¨ç¢¼å¤±æ•—: {e}")
                # è¿”å›éš¨æ©Ÿç‰¹å¾µ
                return torch.randn(1, 768, device=self.device)
        else:
            # ä½¿ç”¨ç°¡å–®çš„ç‰¹å¾µæå–
            return torch.randn(1, 768, device=self.device)
    
    def predict_quality(self, text: str) -> float:
        """é æ¸¬æ–‡æœ¬å“è³ªåˆ†æ•¸"""
        if self.quality_model and self.tokenizer:
            try:
                inputs = self.tokenizer(text,
                                      truncation=True,
                                      padding=True,
                                      max_length=512,
                                      return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.quality_model(**inputs)
                    score = outputs.logits.item()
                
                return max(0.0, min(2.0, score))  # é™åˆ¶åœ¨ 0-2 ç¯„åœ
            except Exception as e:
                print(f"âŒ å“è³ªé æ¸¬å¤±æ•—: {e}")
                return 0.5
        else:
            # ç°¡å–®çš„å•Ÿç™¼å¼è©•åˆ†
            return self._heuristic_quality(text)
    
    def _heuristic_quality(self, text: str) -> float:
        """å•Ÿç™¼å¼å“è³ªè©•åˆ†"""
        score = 0.5  # åŸºç¤åˆ†
        
        # é•·åº¦è©•åˆ†
        word_count = len(text.split())
        if 50 <= word_count <= 150:
            score += 0.2
        elif word_count < 20:
            score -= 0.2
        
        # çµæ§‹è©•åˆ†
        if any(marker in text for marker in ['because', 'therefore', 'however', 'furthermore']):
            score += 0.15
        
        # è­‰æ“šè©•åˆ†
        if any(marker in text for marker in ['study', 'research', 'data', 'evidence']):
            score += 0.1
        
        # å¼•ç”¨è©•åˆ†
        if '[CITE]' in text or re.search(r'\[\d+\]', text):
            score += 0.1
        
        return max(0.0, min(2.0, score))
    
    def select_strategy(self, query: str, context: str = "", social_context: List[float] = None) -> str:
        """é¸æ“‡è¾¯è«–ç­–ç•¥"""
        try:
            # ç·¨ç¢¼è¼¸å…¥
            query_features = self.encode_text(query + " " + context)
            
            # è™•ç†ç¤¾æœƒèƒŒæ™¯
            if social_context:
                social_tensor = torch.tensor(social_context, device=self.device).unsqueeze(0)
            else:
                social_tensor = None
            
            # ç­–ç•¥é æ¸¬
            with torch.no_grad():
                outputs = self.policy_model(query_features, social_tensor)
                strategy_probs = torch.softmax(outputs['strategy_logits'], dim=-1)
                strategy_idx = torch.argmax(strategy_probs, dim=-1).item()
            
            selected_strategy = self.strategies[strategy_idx]
            confidence = strategy_probs[0][strategy_idx].item()
            
            print(f"ğŸ¯ é¸æ“‡ç­–ç•¥: {selected_strategy} (ä¿¡å¿ƒåº¦: {confidence:.3f})")
            return selected_strategy
            
        except Exception as e:
            print(f"âŒ ç­–ç•¥é¸æ“‡å¤±æ•—: {e}")
            return 'analytical'  # é è¨­ç­–ç•¥

def choose_snippet(state_text: str, pool: List[Dict], policy_net: PolicyNetwork = None) -> str:
    """é¸æ“‡æœ€ä½³çš„è­‰æ“šç‰‡æ®µ"""
    
    if not pool:
        return "No evidence available."
    
    # åˆå§‹åŒ–ç­–ç•¥ç¶²è·¯
    if policy_net is None:
        policy_net = PolicyNetwork()
    
    try:
        # ç·¨ç¢¼æŸ¥è©¢æ–‡æœ¬
        query_features = policy_net.encode_text(state_text)
        
        # è©•ä¼°æ¯å€‹ç‰‡æ®µ
        snippet_scores = []
        for snippet in pool:
            content = snippet.get('content', '')
            
            # ç·¨ç¢¼ç‰‡æ®µ
            snippet_features = policy_net.encode_text(content)
            
            # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
            with torch.no_grad():
                outputs = policy_net.policy_model(
                    query_features, 
                    snippet_features=snippet_features
                )
                relevance_score = torch.sigmoid(outputs['ranking_score']).item()
            
            # é æ¸¬å“è³ªåˆ†æ•¸
            quality_score = policy_net.predict_quality(content)
            
            # ç¶œåˆåˆ†æ•¸
            original_score = snippet.get('similarity_score', 0.0)
            combined_score = (
                0.4 * relevance_score + 
                0.3 * quality_score + 
                0.3 * original_score
            )
            
            snippet_scores.append({
                'content': content,
                'score': combined_score,
                'relevance': relevance_score,
                'quality': quality_score,
                'original': original_score,
                'metadata': snippet
            })
        
        # æ’åºä¸¦é¸æ“‡æœ€ä½³ç‰‡æ®µ
        snippet_scores.sort(key=lambda x: x['score'], reverse=True)
        best_snippet = snippet_scores[0]
        
        print(f"ğŸ“ é¸æ“‡æœ€ä½³ç‰‡æ®µ (åˆ†æ•¸: {best_snippet['score']:.3f})")
        print(f"   - ç›¸é—œæ€§: {best_snippet['relevance']:.3f}")
        print(f"   - å“è³ª: {best_snippet['quality']:.3f}")
        print(f"   - åŸå§‹: {best_snippet['original']:.3f}")
        
        return best_snippet['content']
        
    except Exception as e:
        print(f"âŒ ç‰‡æ®µé¸æ“‡å¤±æ•—: {e}")
        # å›é€€åˆ°ç°¡å–®é¸æ“‡
        return max(pool, key=lambda x: x.get('similarity_score', 0.0))['content']

# å…¨åŸŸç­–ç•¥ç¶²è·¯å¯¦ä¾‹
_global_policy_net = None

def get_policy_network() -> PolicyNetwork:
    """ç²å–å…¨åŸŸç­–ç•¥ç¶²è·¯å¯¦ä¾‹"""
    global _global_policy_net
    if _global_policy_net is None:
        _global_policy_net = PolicyNetwork()
    return _global_policy_net

def select_strategy(query: str, context: str = "", social_context: List[float] = None) -> str:
    """é¸æ“‡è¾¯è«–ç­–ç•¥ï¼ˆå¤–éƒ¨æ¥å£ï¼‰"""
    policy_net = get_policy_network()
    return policy_net.select_strategy(query, context, social_context)

# ä¸»è¦å¤–éƒ¨æ¥å£
__all__ = ['choose_snippet', 'select_strategy', 'PolicyNetwork', 'DebatePolicy']
