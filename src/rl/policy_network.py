"""
RL Policy Network for Debate Strategy
ä½¿ç”¨ PPO è¨“ç·´çš„ç­–ç•¥ç¶²è·¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import numpy as np
from transformers import AutoTokenizer, AutoModel

# å°å…¥ PPO ç›¸é—œé¡
from .ppo_trainer import DebatePPONetwork

class PolicyNetwork:
    """è¾¯è«–ç­–ç•¥ç¶²è·¯ï¼ˆä½¿ç”¨ PPOï¼‰"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"PolicyNetwork ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # è¼‰å…¥ tokenizer å’Œ encoder
        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
        self.encoder = AutoModel.from_pretrained('distilbert-base-uncased')
        self.encoder = self.encoder.to(self.device)
        self.encoder.eval()
        
        # ç­–ç•¥åˆ—è¡¨
        self.strategies = ['aggressive', 'defensive', 'analytical', 'empathetic']
        
        # è¼‰å…¥ PPO æ¨¡å‹
        self.policy_model = DebatePPONetwork(
            state_dim=768,
            hidden_dim=256,
            num_actions=4
        ).to(self.device)
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾‘ï¼Œè¼‰å…¥é è¨“ç·´æ¬Šé‡
        if model_path and Path(model_path).exists():
            self.load_model(model_path)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°é è¨“ç·´ PPO æ¨¡å‹ï¼Œä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–")
    
    def load_model(self, model_path: str):
        """è¼‰å…¥é è¨“ç·´æ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            self.policy_model.load_state_dict(checkpoint['policy_state_dict'])
            self.policy_model.eval()
            print(f"âœ… è¼‰å…¥ PPO æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
    
    def encode_state(self, 
                    query: str, 
                    context: str = "", 
                    social_context: Optional[List[float]] = None,
                    debate_history: Optional[List[Dict]] = None) -> torch.Tensor:
        """ç·¨ç¢¼è¾¯è«–ç‹€æ…‹"""
        # ç·¨ç¢¼æ–‡æœ¬
        text = f"{query} {context}".strip()
        with torch.no_grad():
            inputs = self.tokenizer(text, truncation=True, max_length=512, 
                                  padding=True, return_tensors='pt')
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.encoder(**inputs)
            text_features = outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()
        
        # å‰µå»ºç‹€æ…‹å‘é‡
        state = np.zeros(768)
        state[:len(text_features)] = text_features
        
        # æ·»åŠ ç¤¾æœƒèƒŒæ™¯ä¿¡æ¯
        if social_context and len(social_context) >= 2:
            state[0] = social_context[0]  # ç•¶å‰ç«‹å ´
            state[1] = social_context[1]  # å°æ‰‹ç«‹å ´
        
        # æ·»åŠ æ­·å²ä¿¡æ¯
        if debate_history:
            recent_strategies = [h.get('strategy', 'analytical') for h in debate_history[-5:]]
            for i, strategy in enumerate(recent_strategies):
                if strategy in self.strategies:
                    state[10 + i] = self.strategies.index(strategy) / 4.0
        
        return torch.tensor(state, dtype=torch.float32)
    
    def select_strategy(self, 
                       query: str, 
                       context: str = "", 
                       social_context: Optional[List[float]] = None,
                       debate_history: Optional[List[Dict]] = None,
                       deterministic: bool = False) -> str:
        """é¸æ“‡è¾¯è«–ç­–ç•¥"""
        # ç·¨ç¢¼ç‹€æ…‹
        state = self.encode_state(query, context, social_context, debate_history)
        state = state.unsqueeze(0).to(self.device)
        
        # ä½¿ç”¨ PPO æ¨¡å‹é¸æ“‡ç­–ç•¥
        with torch.no_grad():
            action, _, _ = self.policy_model.get_action(state, deterministic=deterministic)
        
        selected_strategy = self.strategies[action]
        print(f"ğŸ¯ PPO é¸æ“‡ç­–ç•¥: {selected_strategy}")
        
        return selected_strategy
    
    def get_strategy_distribution(self, 
                                 query: str, 
                                 context: str = "", 
                                 social_context: Optional[List[float]] = None,
                                 debate_history: Optional[List[Dict]] = None) -> Dict[str, float]:
        """ç²å–ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ"""
        # ç·¨ç¢¼ç‹€æ…‹
        state = self.encode_state(query, context, social_context, debate_history)
        state = state.unsqueeze(0).to(self.device)
        
        # ç²å–å‹•ä½œæ¦‚ç‡
        with torch.no_grad():
            action_logits, _ = self.policy_model(state)
            action_probs = F.softmax(action_logits, dim=-1).squeeze().cpu().numpy()
        
        # å‰µå»ºç­–ç•¥åˆ†å¸ƒå­—å…¸
        strategy_dist = {
            strategy: float(prob) 
            for strategy, prob in zip(self.strategies, action_probs)
        }
        
        return strategy_dist
    
    def predict_quality(self, text: str) -> float:
        """é æ¸¬æ–‡æœ¬å“è³ªï¼ˆä½¿ç”¨åƒ¹å€¼å‡½æ•¸ï¼‰"""
        # ç·¨ç¢¼ç‹€æ…‹
        state = self.encode_state(text)
        state = state.unsqueeze(0).to(self.device)
        
        # ä½¿ç”¨åƒ¹å€¼å‡½æ•¸é æ¸¬
        with torch.no_grad():
            _, value = self.policy_model(state)
            quality_score = torch.sigmoid(value).item()  # è½‰æ›åˆ° 0-1 ç¯„åœ
        
        return quality_score

# å…¨å±€å¯¦ä¾‹
_policy_network = None

def get_policy_network(model_path: Optional[str] = None) -> PolicyNetwork:
    """ç²å–ç­–ç•¥ç¶²è·¯å¯¦ä¾‹"""
    global _policy_network
    
    if _policy_network is None:
        # å˜—è©¦è¼‰å…¥é è¨“ç·´æ¨¡å‹
        if model_path is None:
            model_path = "data/models/ppo/ppo_debate_policy.pt"
        
        _policy_network = PolicyNetwork(model_path)
    
    return _policy_network

def select_strategy(query: str, 
                   context: str = "", 
                   social_context: Optional[List[float]] = None,
                   debate_history: Optional[List[Dict]] = None) -> str:
    """é¸æ“‡è¾¯è«–ç­–ç•¥ï¼ˆä¾¿æ·å‡½æ•¸ï¼‰"""
    network = get_policy_network()
    return network.select_strategy(query, context, social_context, debate_history)

def choose_snippet(query: str, 
                  snippets: List[str], 
                  context: str = "",
                  strategy: Optional[str] = None) -> str:
    """é¸æ“‡æœ€ä½³ç‰‡æ®µ"""
    if not snippets:
        return ""
    
    network = get_policy_network()
    
    # è©•ä¼°æ¯å€‹ç‰‡æ®µçš„å“è³ª
    scores = []
    for snippet in snippets:
        combined_text = f"{query} {context} {snippet}"
        score = network.predict_quality(combined_text)
        
        # æ ¹æ“šç­–ç•¥èª¿æ•´åˆ†æ•¸
        if strategy:
            if strategy == 'aggressive' and any(word in snippet.lower() for word in ['wrong', 'incorrect', 'flawed']):
                score *= 1.2
            elif strategy == 'empathetic' and any(word in snippet.lower() for word in ['understand', 'appreciate', 'feel']):
                score *= 1.2
            elif strategy == 'analytical' and any(word in snippet.lower() for word in ['analyze', 'consider', 'examine']):
                score *= 1.2
        
        scores.append(score)
    
    # é¸æ“‡æœ€é«˜åˆ†çš„ç‰‡æ®µ
    best_idx = np.argmax(scores)
    print(f"ğŸ“„ é¸æ“‡ç‰‡æ®µ {best_idx + 1}/{len(snippets)}ï¼Œåˆ†æ•¸: {scores[best_idx]:.3f}")
    
    return snippets[best_idx]
